{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4e062b",
   "metadata": {},
   "source": [
    "## Deep Learning Approaches for RF-based detection & classification\n",
    "Transfer learning with a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df87784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "# import the torch packages\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import custom functions\n",
    "from helper_functions import *\n",
    "from latency_helpers import *\n",
    "from loading_functions import *\n",
    "\n",
    "from Torch_Models import *\n",
    "\n",
    "from file_paths import *\n",
    "\n",
    "from nn_functions import runkfoldcv\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dacfef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import Torch_Models\n",
    "importlib.reload(Torch_Models)\n",
    "from Torch_Models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4835a",
   "metadata": {},
   "source": [
    "### Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7710a0b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAW dronedetect data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 1451.69it/s]\n",
      "get file lengths: 100%|████████████████████████████████████████████████████████████████████████████████████████| 390/390 [00:00<00:00, 754858.59it/s]\n"
     ]
    }
   ],
   "source": [
    "feat_folder = '../Features/'\n",
    "feat_name = 'RAW' # or RAW\n",
    "t_seg = 20\n",
    "n_per_seg = 1024\n",
    "\n",
    "output_name = 'drones'\n",
    "feat_format = 'IMG'\n",
    "which_dataset = 'dronedetect'\n",
    "\n",
    "# dataset specific parameters\n",
    "drrf_highlow = 'H'\n",
    "drde_ints = ['WIFI','CLEAN','BLUE','BOTH']\n",
    "\n",
    "if feat_name == 'RAW':\n",
    "    print ('Loading RAW '+ which_dataset +' data')\n",
    "    if which_dataset == 'dronerf':\n",
    "        dataset = RFRawTorch(dronerf_raw_path, t_seg, output_name, which_dataset, \n",
    "                             downsamp=10000, interferences=None)\n",
    "    elif which_dataset == 'dronedetect':\n",
    "        dataset = RFRawTorch(dronedetect_raw_path, t_seg, output_name, which_dataset, \n",
    "                             downsamp=10000, interferences=drde_ints)\n",
    "else:\n",
    "    if which_dataset == 'dronerf':\n",
    "        print('Loading DroneRF Dataset')\n",
    "        dataset = DroneRFTorch(dronerf_feat_path, feat_name, t_seg, n_per_seg,\n",
    "                           feat_format, output_name, drrf_highlow)\n",
    "    elif which_dataset == 'dronedetect':\n",
    "        print('Loading DroneDetect Dataset')\n",
    "        dataset = DroneDetectTorch(dronedetect_feat_path, feat_name, t_seg, n_per_seg, feat_format,\n",
    "                                        output_name, drde_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ceb672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  2.5,  5. ,  7.5, 10. ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 10, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bf51c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10000])\n"
     ]
    }
   ],
   "source": [
    "feat, label = dataset.__getitem__(100)\n",
    "# plt.imshow(feat)\n",
    "# plt.show()\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb467a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650b6fe",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a9689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dataset.unique_labels)\n",
    "which_model = '1dconv' # or 'resnet'\n",
    "if which_model == 'vgg':\n",
    "    Model = VGGFC(num_classes)\n",
    "elif which_model == 'resnet':\n",
    "    Model = ResNetFC(num_classes)\n",
    "elif which_model == '1dconv':\n",
    "    Model = RFUAVNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samp = dataset.__getitem__(40)[0]\n",
    "test_samp = torch.unsqueeze(test_samp, 0)\n",
    "Model(test_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7134f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "k_folds = 5\n",
    "\n",
    "batch_size = 128 # 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1 # 0\n",
    "momentum = 0.95\n",
    "l2reg = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54e3bfc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "('MIN', 'MIN', 'PHA', 'MA1', 'AIR', 'PHA', 'AIR', 'PHA', 'DIS', 'DIS', 'MA1', 'DIS', 'AIR', 'INS', 'PHA', 'PHA', 'DIS', 'PHA', 'INS', 'PHA', 'MA1', 'PHA', 'AIR', 'MAV', 'MA1', 'PHA', 'DIS', 'AIR', 'MIN', 'MIN', 'AIR', 'PHA', 'MIN', 'DIS', 'MAV', 'MIN', 'AIR', 'MIN', 'MAV', 'MAV', 'AIR', 'MAV', 'MIN', 'PHA', 'MIN', 'INS', 'MIN', 'INS', 'PHA', 'INS', 'INS', 'MA1', 'AIR', 'MA1', 'DIS', 'DIS', 'DIS', 'AIR', 'DIS', 'MAV', 'MIN', 'DIS', 'AIR', 'AIR', 'MIN', 'MA1', 'AIR', 'MA1', 'AIR', 'MIN', 'AIR', 'AIR', 'MIN', 'AIR', 'AIR', 'AIR', 'MIN', 'MIN', 'AIR', 'MAV', 'MAV', 'MIN', 'MIN', 'MA1', 'MAV', 'MAV', 'MAV', 'MA1', 'PHA', 'MAV', 'MAV', 'AIR', 'MIN', 'INS', 'MA1', 'MAV', 'MA1', 'INS', 'MIN', 'MAV', 'MA1', 'INS', 'MIN', 'INS', 'AIR', 'MIN', 'AIR', 'MIN', 'MA1', 'PHA', 'MIN', 'MA1', 'MA1', 'PHA', 'MAV', 'PHA', 'INS', 'MIN', 'INS', 'MAV', 'MA1', 'PHA', 'MAV', 'DIS', 'MAV', 'MA1', 'AIR', 'MAV')\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1981274/4105186653.py\", line 1, in <cell line: 1>\n",
      "    trainedModel, res_acc, res_f1, res_runtime = runkfoldcv(Model, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg)\n",
      "  File \"/home/kzhou/main/RFClassification/nn_functions.py\", line 72, in runkfoldcv\n",
      "    targets= targets.type(torch.long)\n",
      "AttributeError: 'tuple' object has no attribute 'type'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/kzhou/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "trainedModel, res_acc, res_f1, res_runtime = runkfoldcv(Model, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b978843",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e05261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c5339d",
   "metadata": {},
   "source": [
    "### Load gamutRF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be57236",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GamutRFDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m save_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     15\u001b[0m eval_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m---> 17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mGamutRFDataset\u001b[49m(label_dirs, sample_secs\u001b[38;5;241m=\u001b[39msample_secs, nfft\u001b[38;5;241m=\u001b[39mnfft)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GamutRFDataset' is not defined"
     ]
    }
   ],
   "source": [
    "label_dirs= {\n",
    "    'drone': ['data/gamutrf-birdseye-field-days/leesburg_field_day_2022_06_15/worker1/','data/gamutrf-birdseye-field-days/pdx_field_day_2022_05_26/worker1/gamutrf/'], \n",
    "    'wifi_2_4': ['data/gamutrf-pdx/07_21_2022/wifi_2_4/'], \n",
    "    'wifi_5': ['data/gamutrf-pdx/07_21_2022/wifi_5/']\n",
    "}\n",
    "\n",
    "sample_secs = 0.02\n",
    "nfft = 512\n",
    "batch_size = 8\n",
    "num_workers = 19\n",
    "num_epochs = 4\n",
    "train_val_test_split = [0.75, 0.05, 0.20]\n",
    "save_iter = 200\n",
    "eval_iter = 10000\n",
    "\n",
    "dataset = GamutRFDataset(label_dirs, sample_secs=sample_secs, nfft=nfft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad3ec6",
   "metadata": {},
   "source": [
    "## VGG16 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3fd24d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "# summary(vgg16, (3,224,224))\n",
    "\n",
    "modules=list(vgg16.children())[:-1]\n",
    "vggmodel=nn.Sequential(*modules)\n",
    "\n",
    "for p in vggmodel.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81a79389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one input\n",
    "d = dataset.__getitem__(11)\n",
    "inarr = d[0]\n",
    "inarr = torch.moveaxis(inarr, 2, 0)\n",
    "inarr =inarr.unsqueeze(dim=0)\n",
    "print(inarr.shape)\n",
    "# print(inarr.shape)\n",
    "# postmove = inarr[0]\n",
    "# print(premove==postmove) # confirm it is the same channel\n",
    "\n",
    "# WHEN using single channel array format\n",
    "# inputr = inarr.repeat(1,3,1,1)\n",
    "# inputr = inputr.to(device)\n",
    "out = vggmodel(inarr)\n",
    "\n",
    "# reshape the output\n",
    "out.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e445e2e",
   "metadata": {},
   "source": [
    "## Resnet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47b6d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transfer learning from Resnet50 & Apply Logistic Regression (Swinney paper)\n",
    "\n",
    "# use pretrained resnet feature and just keep up to the last layer\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "modules=list(resnet50.children())[:-2]\n",
    "resnet50=nn.Sequential(*modules)\n",
    "for p in resnet50.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "440b23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 7, 7])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test resnet\n",
    "# input = torch.randn(1,1,30,300)\n",
    "d = dataset.__getitem__(0)\n",
    "inarr = d[0]\n",
    "\n",
    "inarr = torch.moveaxis(inarr, 2, 0)\n",
    "inarr = inarr.unsqueeze(dim=0)\n",
    "print(inarr.shape)\n",
    "# resnet50(inarr).shape\n",
    "\n",
    "resnet50(inarr).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce13e9",
   "metadata": {},
   "source": [
    "### Generate Pretrained/Transfer Learning Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85a4b583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 36778/36778 [02:11<00:00, 279.85it/s]\n"
     ]
    }
   ],
   "source": [
    "ModelDict = {'vgg':vggmodel, 'resnet': resnet50}\n",
    "\n",
    "which_model = 'vgg'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ModelDict[which_model] = ModelDict[which_model].to(device) #set model to device\n",
    "\n",
    "Feats = []\n",
    "y_num = [] # numerical values for y\n",
    "for n in tqdm(range(len(dataset))):\n",
    "    d = dataset.__getitem__(n)\n",
    "    inarr = d[0]\n",
    "#     inputr = inarr.repeat(1,3,1,1)  # repeat to have 3 channels of the same info\n",
    "    inputr = torch.moveaxis(inarr, 2, 0) # move axis to have channels come first\n",
    "    inputr =inputr.unsqueeze(dim=0)\n",
    "    inputr = inputr.to(device)\n",
    "    out = ModelDict[which_model](inputr)\n",
    "    \n",
    "    Feats.append(out.cpu().numpy().flatten())\n",
    "    y_num.append(np.array(d[1]))\n",
    "\n",
    "Feats = np.array(Feats)\n",
    "y_num = np.array(y_num)\n",
    "\n",
    "# flatten the middle dimension\n",
    "Feats = Feats.reshape(Feats.shape[0], Feats.shape[-1])\n",
    "# invert labels back to categorical\n",
    "# vgg_y_cat = dataset.le.inverse_transform(vgg_y.astype(np.int64))\n",
    "y_cat = np.array([dataset.idx_to_class[i] for i in y_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4da21074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save VGG features\n",
    "# vgg_save = {'feats': vgg_feats, 'y_cat':vgg_y_cat, 'y':vgg_y}\n",
    "# file_name = 'transfer_learning_feats/VggFeats_'+str(seg_len)+'_'+str(n_per_seg)+'correct_psd_img'\n",
    "# np.save(file_name, vgg_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be6ec2",
   "metadata": {},
   "source": [
    "### Run Classifier (LR or kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b769a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Features: (n sample x n feats): (36778, 25088)\n"
     ]
    }
   ],
   "source": [
    "Xs_feat = Feats # which features to use for logit reg\n",
    "y_cat = y_cat\n",
    "print('Shape of Input Features: (n sample x n feats):', Xs_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f07bbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KFOLD split\n",
    "\n",
    "k_fold = 5\n",
    "cv = KFold(n_splits=k_fold, random_state=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab9544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [09:03, 543.57s/it]"
     ]
    }
   ],
   "source": [
    "# Apply PCA with LR and kNN\n",
    "sc_ls = []\n",
    "f1_ls = []\n",
    "tm_ls = []\n",
    "which_clf = 'kNN'\n",
    "for train_ix, test_ix in tqdm(cv.split(Xs_feat)):\n",
    "    scaler = StandardScaler()\n",
    "    Xtrainscale = scaler.fit_transform(Xs_feat[train_ix])\n",
    "    \n",
    "    pca = PCA(n_components=10000)\n",
    "    pca.fit(Xs_feat[train_ix])\n",
    "    Xtrainpca = pca.transform(Xtrainscale)\n",
    "    \n",
    "    if which_clf == 'lr':\n",
    "        clf = LogisticRegression(max_iter=10000)\n",
    "    else:\n",
    "        clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    clf.fit(Xtrainpca, y_cat[train_ix])\n",
    "    \n",
    "    # test\n",
    "    start = time.time()\n",
    "    Xtestscale = scaler.transform(Xs_feat[test_ix])\n",
    "    Xtestpca = pca.transform(Xtestscale)\n",
    "    y_pred = clf.predict(Xtestpca)\n",
    "    end = time.time()\n",
    "    \n",
    "    t_ave = (end-start)/Xtestpca.shape[0] # batch measure time\n",
    "    tm_ls.append(t_ave)\n",
    "    \n",
    "    sc = accuracy_score(y_cat[test_ix], y_pred)\n",
    "    sc_ls.append(sc)\n",
    "    f1 = f1_score(y_cat[test_ix], y_pred, average='weighted')\n",
    "    f1_ls.append(f1)\n",
    "\n",
    "out_msg = 'Net+LR: average test acc: {:.3}, F1: {:.3}, Run-time: {:.3}ms'.format(np.mean(sc_ls), np.mean(f1_ls), np.mean(tm_ls)*1e3)\n",
    "print(out_msg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75156f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0005148707321816803,\n",
       " 0.0005234912641847237,\n",
       " 0.0005121750670130158,\n",
       " 0.0005138173020750074,\n",
       " 0.0005133497936745498]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3eea904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Directly apply LR - too many features too long\n",
    "\n",
    "# model parameters\n",
    "# Cs=list(map(lambda x:pow(10,x),range(-2,2,1)))\n",
    "# print('Cs:', Cs)\n",
    "\n",
    "# best_params_ls = []\n",
    "# acc_ls = []\n",
    "# f1_ls = []\n",
    "# runt_ls = []\n",
    "\n",
    "# parameters = {'C':Cs}\n",
    "\n",
    "# for train_ix, test_ix in tqdm(cv.split(Xs_feat)):\n",
    "    \n",
    "#     # find the optimal hypber parameters\n",
    "#     lr = LogisticRegression(solver='saga')\n",
    "# #     clf = GridSearchCV(lr, parameters, n_jobs=1) # gridsearch cv\n",
    "#     clf = LogisticRegression(C =1.0, max_iter=5000, class_weight = 'balanced',n_jobs=1) # fixed parameter\n",
    "    \n",
    "#     scaler = preprocessing.StandardScaler().fit(Xs_feat[train_ix])\n",
    "#     X_train_scale = scaler.transform(Xs_feat[train_ix])\n",
    "    \n",
    "#     clf.fit(X_train_scale, y_cat[train_ix])\n",
    "    \n",
    "# #     print(clf.best_params_)\n",
    "# #     best_params_ls.append(clf.best_params_)\n",
    "    \n",
    "#     # predict on the test data\n",
    "#     X_test_scale = scaler.transform(Xs_feat[test_ix])\n",
    "#     y_pred, runtimes = atomic_benchmark_estimator(clf, X_test_scale, output_type= '<U3', \n",
    "#                                                   verbose=False)\n",
    "#     runt_ls.append(np.mean(runtimes))\n",
    "    \n",
    "#     acc = accuracy_score(y_cat[test_ix], y_pred)\n",
    "#     f1 = f1_score(y_cat[test_ix], y_pred, average='weighted')\n",
    "#     print('Accuracy: {:.3},\\t F1: {:.3}'.format(acc,f1))\n",
    "#     acc_ls.append(acc)\n",
    "#     f1_ls.append(f1)\n",
    "    \n",
    "# out_msg = 'Net+LR: average test acc: {:.2}, F1: {:.2}, Run-time: {:.2}ms'.format(np.mean(acc_ls), np.mean(f1_ls), np.mean(runt_ls)*1e3)\n",
    "# print(out_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce4b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# neigh = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfedc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of neighbours: [100]\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameter kNN (LONG run time - too many features)\n",
    "# k_fold = 5\n",
    "# cv = KFold(n_splits=k_fold, random_state=10, shuffle=True)\n",
    "\n",
    "# # Ns=list(range(2,100,20))\n",
    "# # Ns = [100]\n",
    "# parameters = {'n_neighbors':Ns}\n",
    "# print('list of neighbours:', Ns)\n",
    "\n",
    "# Xs_arr = Vgg_Feats\n",
    "# y_arr = Vgg_y_cat\n",
    "\n",
    "# best_params_ls = []\n",
    "# score_ls = []\n",
    "# f1_ls = []\n",
    "\n",
    "# for train_ix, test_ix in cv.split(Xs_arr):\n",
    "#     # scale data\n",
    "#     scaler = preprocessing.StandardScaler().fit(Xs_arr[train_ix])\n",
    "#     X_train_scale = scaler.transform(Xs_arr[train_ix])\n",
    "    \n",
    "#     # find the optimal hypber parameters\n",
    "#     clf = KNeighborsClassifier(n_neighbors=5)\n",
    "# #     clf = GridSearchCV(neigh, parameters, n_jobs=1)\n",
    "#     clf.fit(X_train_scale, y_arr[train_ix])\n",
    "# #     print(clf.best_parameters)\n",
    "    \n",
    "#     # predict on the test data\n",
    "#     X_test_scale = scaler.transform(Xs_arr[test_ix])\n",
    "# #     y_pred = clf.predict(X_test_scale)\n",
    "#     y_pred, runtimes = atomic_benchmark_estimator(clf, X_test_scale, output_type= '<U3', \n",
    "#                                                   verbose=False)\n",
    "#     acc = accuracy_score(y_arr[test_ix], y_pred)\n",
    "#     f1 = f1_score(y_arr[test_ix], y_pred, average='weighted')\n",
    "#     f1_ls.append(f1)\n",
    "#     print('Accuracy: {:.3},\\t F1: {:.3}'.format(acc,f1))\n",
    "#     score_ls.append(acc)\n",
    "    \n",
    "# print('VGG feats+kNN K-fold average test score:', np.mean(score_ls))\n",
    "# print('VGG feats+kNN K-fold average test F1:', np.mean(f1_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80076188",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b356583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Torch models\n",
    "from Torch_Models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fdafd",
   "metadata": {},
   "source": [
    "### Kfold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1f7ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f67796ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "k_folds = 5\n",
    "\n",
    "batch_size = 128 # 128\n",
    "num_classes = 7\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # 0\n",
    "momentum = 0.95\n",
    "l2reg = 1e-4\n",
    "\n",
    "Model = VGGFC(num_classes)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ece73702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.21611\n",
      "    Loss after mini-batch   100: 0.23081\n",
      "    Loss after mini-batch   150: 0.22870\n",
      "    Loss after mini-batch   200: 0.23318\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.22025\n",
      "    Loss after mini-batch   100: 0.21542\n",
      "    Loss after mini-batch   150: 0.22238\n",
      "    Loss after mini-batch   200: 0.22282\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.21271\n",
      "    Loss after mini-batch   100: 0.21916\n",
      "    Loss after mini-batch   150: 0.21282\n",
      "    Loss after mini-batch   200: 0.21872\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.20875\n",
      "    Loss after mini-batch   100: 0.21702\n",
      "    Loss after mini-batch   150: 0.21105\n",
      "    Loss after mini-batch   200: 0.20857\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.20477\n",
      "    Loss after mini-batch   100: 0.20615\n",
      "    Loss after mini-batch   150: 0.21356\n",
      "    Loss after mini-batch   200: 0.20486\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.20678\n",
      "    Loss after mini-batch   100: 0.20490\n",
      "    Loss after mini-batch   150: 0.20323\n",
      "    Loss after mini-batch   200: 0.20357\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.20139\n",
      "    Loss after mini-batch   100: 0.20259\n",
      "    Loss after mini-batch   150: 0.20221\n",
      "    Loss after mini-batch   200: 0.21059\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.19075\n",
      "    Loss after mini-batch   100: 0.20089\n",
      "    Loss after mini-batch   150: 0.20219\n",
      "    Loss after mini-batch   200: 0.20622\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.20032\n",
      "    Loss after mini-batch   100: 0.19470\n",
      "    Loss after mini-batch   150: 0.19704\n",
      "    Loss after mini-batch   200: 0.19810\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.19359\n",
      "    Loss after mini-batch   100: 0.19233\n",
      "    Loss after mini-batch   150: 0.19559\n",
      "    Loss after mini-batch   200: 0.19293\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 0: 91.64 %\n",
      "F1 for fold 0: 0.92 \n",
      "Runtime for fold 0: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.20640\n",
      "    Loss after mini-batch   100: 0.20344\n",
      "    Loss after mini-batch   150: 0.20764\n",
      "    Loss after mini-batch   200: 0.20371\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.19874\n",
      "    Loss after mini-batch   100: 0.19279\n",
      "    Loss after mini-batch   150: 0.20458\n",
      "    Loss after mini-batch   200: 0.19378\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.19644\n",
      "    Loss after mini-batch   100: 0.20087\n",
      "    Loss after mini-batch   150: 0.19580\n",
      "    Loss after mini-batch   200: 0.19586\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.18543\n",
      "    Loss after mini-batch   100: 0.19630\n",
      "    Loss after mini-batch   150: 0.19607\n",
      "    Loss after mini-batch   200: 0.19530\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.18489\n",
      "    Loss after mini-batch   100: 0.19480\n",
      "    Loss after mini-batch   150: 0.19048\n",
      "    Loss after mini-batch   200: 0.18585\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.18172\n",
      "    Loss after mini-batch   100: 0.18546\n",
      "    Loss after mini-batch   150: 0.19167\n",
      "    Loss after mini-batch   200: 0.18302\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.18040\n",
      "    Loss after mini-batch   100: 0.18397\n",
      "    Loss after mini-batch   150: 0.17943\n",
      "    Loss after mini-batch   200: 0.19092\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.18657\n",
      "    Loss after mini-batch   100: 0.18231\n",
      "    Loss after mini-batch   150: 0.17525\n",
      "    Loss after mini-batch   200: 0.18513\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.18527\n",
      "    Loss after mini-batch   100: 0.17363\n",
      "    Loss after mini-batch   150: 0.17686\n",
      "    Loss after mini-batch   200: 0.18654\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.17766\n",
      "    Loss after mini-batch   100: 0.17687\n",
      "    Loss after mini-batch   150: 0.18324\n",
      "    Loss after mini-batch   200: 0.17613\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 1: 92.87 %\n",
      "F1 for fold 1: 0.93 \n",
      "Runtime for fold 1: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.18953\n",
      "    Loss after mini-batch   100: 0.18278\n",
      "    Loss after mini-batch   150: 0.19074\n",
      "    Loss after mini-batch   200: 0.18323\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.18251\n",
      "    Loss after mini-batch   100: 0.17989\n",
      "    Loss after mini-batch   150: 0.18577\n",
      "    Loss after mini-batch   200: 0.18109\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.17309\n",
      "    Loss after mini-batch   100: 0.18050\n",
      "    Loss after mini-batch   150: 0.18379\n",
      "    Loss after mini-batch   200: 0.17434\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.17085\n",
      "    Loss after mini-batch   100: 0.17058\n",
      "    Loss after mini-batch   150: 0.18564\n",
      "    Loss after mini-batch   200: 0.17604\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.16926\n",
      "    Loss after mini-batch   100: 0.17229\n",
      "    Loss after mini-batch   150: 0.17867\n",
      "    Loss after mini-batch   200: 0.16974\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.16943\n",
      "    Loss after mini-batch   100: 0.17833\n",
      "    Loss after mini-batch   150: 0.17062\n",
      "    Loss after mini-batch   200: 0.17068\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.16834\n",
      "    Loss after mini-batch   100: 0.16996\n",
      "    Loss after mini-batch   150: 0.16754\n",
      "    Loss after mini-batch   200: 0.17007\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.16992\n",
      "    Loss after mini-batch   100: 0.15968\n",
      "    Loss after mini-batch   150: 0.16076\n",
      "    Loss after mini-batch   200: 0.16875\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.15917\n",
      "    Loss after mini-batch   100: 0.16284\n",
      "    Loss after mini-batch   150: 0.16923\n",
      "    Loss after mini-batch   200: 0.16468\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.16372\n",
      "    Loss after mini-batch   100: 0.16020\n",
      "    Loss after mini-batch   150: 0.15630\n",
      "    Loss after mini-batch   200: 0.16888\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 2: 94.09 %\n",
      "F1 for fold 2: 0.94 \n",
      "Runtime for fold 2: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.17259\n",
      "    Loss after mini-batch   100: 0.16599\n",
      "    Loss after mini-batch   150: 0.17562\n",
      "    Loss after mini-batch   200: 0.16449\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.16340\n",
      "    Loss after mini-batch   100: 0.16551\n",
      "    Loss after mini-batch   150: 0.16449\n",
      "    Loss after mini-batch   200: 0.16293\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.15863\n",
      "    Loss after mini-batch   100: 0.16278\n",
      "    Loss after mini-batch   150: 0.16199\n",
      "    Loss after mini-batch   200: 0.16354\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.14981\n",
      "    Loss after mini-batch   100: 0.16087\n",
      "    Loss after mini-batch   150: 0.16499\n",
      "    Loss after mini-batch   200: 0.16111\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.15910\n",
      "    Loss after mini-batch   100: 0.15280\n",
      "    Loss after mini-batch   150: 0.15817\n",
      "    Loss after mini-batch   200: 0.15396\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.15089\n",
      "    Loss after mini-batch   100: 0.15658\n",
      "    Loss after mini-batch   150: 0.14915\n",
      "    Loss after mini-batch   200: 0.16400\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.14532\n",
      "    Loss after mini-batch   100: 0.15889\n",
      "    Loss after mini-batch   150: 0.15310\n",
      "    Loss after mini-batch   200: 0.15493\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.15123\n",
      "    Loss after mini-batch   100: 0.15333\n",
      "    Loss after mini-batch   150: 0.15368\n",
      "    Loss after mini-batch   200: 0.15296\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.15109\n",
      "    Loss after mini-batch   100: 0.14296\n",
      "    Loss after mini-batch   150: 0.15182\n",
      "    Loss after mini-batch   200: 0.14831\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.14473\n",
      "    Loss after mini-batch   100: 0.14675\n",
      "    Loss after mini-batch   150: 0.14471\n",
      "    Loss after mini-batch   200: 0.15003\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 3: 94.61 %\n",
      "F1 for fold 3: 0.95 \n",
      "Runtime for fold 3: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.15476\n",
      "    Loss after mini-batch   100: 0.15585\n",
      "    Loss after mini-batch   150: 0.15589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss after mini-batch   200: 0.15825\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.14772\n",
      "    Loss after mini-batch   100: 0.15783\n",
      "    Loss after mini-batch   150: 0.15562\n",
      "    Loss after mini-batch   200: 0.15370\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.15013\n",
      "    Loss after mini-batch   100: 0.15044\n",
      "    Loss after mini-batch   150: 0.14894\n",
      "    Loss after mini-batch   200: 0.15829\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.14647\n",
      "    Loss after mini-batch   100: 0.14365\n",
      "    Loss after mini-batch   150: 0.15970\n",
      "    Loss after mini-batch   200: 0.15232\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.15028\n",
      "    Loss after mini-batch   100: 0.14706\n",
      "    Loss after mini-batch   150: 0.14865\n",
      "    Loss after mini-batch   200: 0.14988\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.14911\n",
      "    Loss after mini-batch   100: 0.14035\n",
      "    Loss after mini-batch   150: 0.14213\n",
      "    Loss after mini-batch   200: 0.14785\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.14468\n",
      "    Loss after mini-batch   100: 0.14567\n",
      "    Loss after mini-batch   150: 0.14202\n",
      "    Loss after mini-batch   200: 0.14324\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.13728\n",
      "    Loss after mini-batch   100: 0.14822\n",
      "    Loss after mini-batch   150: 0.14315\n",
      "    Loss after mini-batch   200: 0.14471\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.12977\n",
      "    Loss after mini-batch   100: 0.14382\n",
      "    Loss after mini-batch   150: 0.14104\n",
      "    Loss after mini-batch   200: 0.14321\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.13555\n",
      "    Loss after mini-batch   100: 0.14104\n",
      "    Loss after mini-batch   150: 0.14148\n",
      "    Loss after mini-batch   200: 0.14219\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 4: 95.66 %\n",
      "F1 for fold 4: 0.96 \n",
      "Runtime for fold 4: 0.0057 s\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 91.63673678809646 %\n",
      "Fold 1: 92.86813750641355 %\n",
      "Fold 2: 94.08671113391482 %\n",
      "Fold 3: 94.6119307248236 %\n",
      "Fold 4: 95.66388710711995 %\n",
      "Average Accuracy: 93.77348065207367 %\n",
      "Average F1: 0.9379559245843282\n",
      "Average Runtime: 0.005712414296929129 s\n"
     ]
    }
   ],
   "source": [
    "trainedModel, res_acc, res_f1, res_runtime = runkfoldcv(Model, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cec40d",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8ef036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as: VGGFC_PSD_1024_20\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model_type = 'VGGFC'\n",
    "model_name = model_type+'_'+str(feat_name)+'_'+str(n_per_seg)+'_'+str(seg_len)\n",
    "model_path = '../saved_models/'\n",
    "trainedModel = trainedModel.cpu()\n",
    "torch.save(trainedModel, model_path+model_name)\n",
    "print('Model saved as:', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195f509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test reload\n",
    "# m2 = torch.load(model_path+model_name)\n",
    "# in2 = dataset.__getitem__(0)[0]\n",
    "# out = m2(in2)\n",
    "# _,pred = torch.max(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a2438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
