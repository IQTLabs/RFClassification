{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4e062b",
   "metadata": {},
   "source": [
    "## Deep Learning Approaches for RF-based detection & classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df87784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "# import the torch packages\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import custom functions\n",
    "from helper_functions import *\n",
    "from latency_helpers import *\n",
    "from loading_functions import *\n",
    "\n",
    "from Torch_Models import *\n",
    "\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4835a",
   "metadata": {},
   "source": [
    "### Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7710a0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Name:  ../Features/IMG_PSD_1024_20/\n"
     ]
    }
   ],
   "source": [
    "feat_folder = '../Features/'\n",
    "feat_name = 'PSD'\n",
    "seg_len = 20\n",
    "n_per_seg = 1024\n",
    "interferences = ['WIFI','CLEAN','BLUE','BOTH']\n",
    "output_name = 'drones'\n",
    "feat_format = 'IMG'\n",
    "\n",
    "dataset = DroneDetectTorch(feat_folder, feat_name, seg_len, n_per_seg, feat_format,\n",
    "                                output_name, interferences)\n",
    "\n",
    "# dataset = load_dronedetect_data(feat_folder, feat_name, seg_len, n_per_seg, feat_format,\n",
    "#                                 output_name, interferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b732f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 38978\n",
      "shape of each item torch.Size([224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "print('dataset size', len(dataset))\n",
    "print('shape of each item', dataset.__getitem__(12)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bf51c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = dataset.__getitem__(4)[0]\n",
    "plt.imshow(d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad3ec6",
   "metadata": {},
   "source": [
    "## VGG16 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fd24d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "# summary(vgg16, (3,224,224))\n",
    "\n",
    "modules=list(vgg16.children())[:-1]\n",
    "vggmodel=nn.Sequential(*modules)\n",
    "\n",
    "for p in vggmodel.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81a79389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one input\n",
    "d = dataset.__getitem__(11)\n",
    "inarr = d[0]\n",
    "inarr = torch.moveaxis(inarr, 2, 0)\n",
    "inarr =inarr.unsqueeze(dim=0)\n",
    "print(inarr.shape)\n",
    "# print(inarr.shape)\n",
    "# postmove = inarr[0]\n",
    "# print(premove==postmove) # confirm it is the same channel\n",
    "\n",
    "# WHEN using single channel array format\n",
    "# inputr = inarr.repeat(1,3,1,1)\n",
    "# inputr = inputr.to(device)\n",
    "out = vggmodel(inarr)\n",
    "\n",
    "# reshape the output\n",
    "out.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e445e2e",
   "metadata": {},
   "source": [
    "## Resnet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47b6d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transfer learning from Resnet50 & Apply Logistic Regression (Swinney paper)\n",
    "\n",
    "# use pretrained resnet feature and just keep up to the last layer\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "modules=list(resnet50.children())[:-2]\n",
    "resnet50=nn.Sequential(*modules)\n",
    "for p in resnet50.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "440b23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 7, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test resnet\n",
    "# input = torch.randn(1,1,30,300)\n",
    "d = dataset.__getitem__(0)\n",
    "inarr = d[0]\n",
    "\n",
    "inarr = torch.moveaxis(inarr, 2, 0)\n",
    "inarr = inarr.unsqueeze(dim=0)\n",
    "print(inarr.shape)\n",
    "# resnet50(inarr).shape\n",
    "\n",
    "resnet50(inarr).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce13e9",
   "metadata": {},
   "source": [
    "### Generate Pretrained/Transfer Learning Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85a4b583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 38978/38978 [02:18<00:00, 280.92it/s]\n"
     ]
    }
   ],
   "source": [
    "ModelDict = {'vgg':vggmodel, 'resnet': resnet50}\n",
    "\n",
    "which_model = 'vgg'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "ModelDict[which_model] = ModelDict[which_model].to(device) #set model to device\n",
    "\n",
    "Feats = []\n",
    "y_num = [] # numerical values for y\n",
    "for n in tqdm(range(len(dataset))):\n",
    "    d = dataset.__getitem__(n)\n",
    "    inarr = d[0]\n",
    "#     inputr = inarr.repeat(1,3,1,1)  # repeat to have 3 channels of the same info\n",
    "    inputr = torch.moveaxis(inarr, 2, 0) # move axis to have channels come first\n",
    "    inputr =inputr.unsqueeze(dim=0)\n",
    "    inputr = inputr.to(device)\n",
    "    out = ModelDict[which_model](inputr)\n",
    "    \n",
    "    Feats.append(out.cpu().numpy().flatten())\n",
    "    y_num.append(np.array(d[1]))\n",
    "\n",
    "Feats = np.array(Feats)\n",
    "y_num = np.array(y_num)\n",
    "\n",
    "# flatten the middle dimension\n",
    "Feats = Feats.reshape(Feats.shape[0], Feats.shape[-1])\n",
    "# invert labels back to categorical\n",
    "# vgg_y_cat = dataset.le.inverse_transform(vgg_y.astype(np.int64))\n",
    "y_cat = np.array([dataset.idx_to_class[i] for i in y_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4da21074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save VGG features\n",
    "# vgg_save = {'feats': vgg_feats, 'y_cat':vgg_y_cat, 'y':vgg_y}\n",
    "# file_name = 'transfer_learning_feats/VggFeats_'+str(seg_len)+'_'+str(n_per_seg)+'correct_psd_img'\n",
    "# np.save(file_name, vgg_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be6ec2",
   "metadata": {},
   "source": [
    "### Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b769a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Input Features: (n sample x n feats): (38978, 25088)\n"
     ]
    }
   ],
   "source": [
    "Xs_feat = Feats # which features to use for logit reg\n",
    "y_cat = y_cat\n",
    "print('Shape of Input Features: (n sample x n feats):', Xs_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f07bbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KFOLD split\n",
    "\n",
    "k_fold = 5\n",
    "cv = KFold(n_splits=k_fold, random_state=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The dimension of data is not reasonable (more features than examples)\n",
    "# Reduce dimension with PCA\n",
    "\n",
    "# pca = PCA()\n",
    "# # Define a Standard Scaler to normalize inputs\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # set the tolerance to a large value to make the example faster\n",
    "# logistic = LogisticRegression(max_iter=10000)\n",
    "# pipe = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"logistic\", logistic)])\n",
    "# print(pipe)\n",
    "# # different parameters\n",
    "# param_grid = {\n",
    "#     \"pca__n_components\": [800],\n",
    "#     \"logistic__C\": [1, 10],\n",
    "# }\n",
    "\n",
    "# best_params_ls = []\n",
    "# acc_ls = []\n",
    "# f1_ls = []\n",
    "# runt_ls = []\n",
    "\n",
    "# for train_ix, test_ix in tqdm(cv.split(Xs_feat)):\n",
    "#     clf = GridSearchCV(pipe, param_grid, n_jobs=1)\n",
    "    \n",
    "#     clf.fit(Xs_feat[train_ix], y_cat[train_ix])\n",
    "    \n",
    "#     print(clf.best_params_)\n",
    "# #     best_params_ls.append(clf.best_params_)\n",
    "    \n",
    "#     # predict on the test data\n",
    "#     y_pred, runtimes = atomic_benchmark_estimator(clf, Xs_feat[test_ix], output_type= '<U3', \n",
    "#                                                   verbose=False)\n",
    "#     runt_ls.append(np.mean(runtimes))\n",
    "    \n",
    "#     acc = accuracy_score(y_cat[test_ix], y_pred)\n",
    "#     f1 = f1_score(y_cat[test_ix], y_pred, average='weighted')\n",
    "#     print('Accuracy: {:.3},\\t F1: {:.3}'.format(acc,f1))\n",
    "#     acc_ls.append(acc)\n",
    "#     f1_ls.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab9544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# test pca\n",
    "sc_ls = []\n",
    "f1_ls = []\n",
    "tm_ls = []\n",
    "for train_ix, test_ix in tqdm(cv.split(Xs_feat)):\n",
    "    scaler = StandardScaler()\n",
    "    Xtrainscale = scaler.fit_transform(Xs_feat[train_ix])\n",
    "    \n",
    "    pca = PCA(n_components=10000)\n",
    "    pca.fit(Xs_feat[train_ix])\n",
    "    Xtrainpca = pca.transform(Xtrainscale)\n",
    "    \n",
    "    lr = LogisticRegression(max_iter=10000)\n",
    "    \n",
    "    lr.fit(Xtrainpca, y_cat[train_ix])\n",
    "    \n",
    "    # test\n",
    "    start = time.time()\n",
    "    Xtestscale = scaler.transform(Xs_feat[test_ix])\n",
    "    Xtestpca = pca.transform(Xtestscale)\n",
    "    y_pred = lr.predict(Xtestpca)\n",
    "    end = time.time()\n",
    "    \n",
    "    t_ave = (end-start)/Xtestpca.shape[0] # batch measure time\n",
    "    tm_ls.append(t_ave)\n",
    "    \n",
    "    sc = accuracy_score(y_cat[test_ix], y_pred)\n",
    "    sc_ls.append(sc)\n",
    "    f1 = f1_score(y_cat[test_ix], y_pred, average='weighted')\n",
    "    f1_ls.append(f1)\n",
    "\n",
    "out_msg = 'Net+LR: average test acc: {:.2}, F1: {:.2}, Run-time: {:.2}ms'.format(np.mean(sc_ls), np.mean(f1_ls), np.mean(tm_ls)*1e3)\n",
    "print(out_msg)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1426b89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8396419565588844"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_cat[test_ix], y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4cd16978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0005148707321816803,\n",
       " 0.0005234912641847237,\n",
       " 0.0005121750670130158,\n",
       " 0.0005138173020750074,\n",
       " 0.0005133497936745498]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eea904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Directly apply LR - too many features too long\n",
    "\n",
    "# model parameters\n",
    "# Cs=list(map(lambda x:pow(10,x),range(-2,2,1)))\n",
    "# print('Cs:', Cs)\n",
    "\n",
    "# best_params_ls = []\n",
    "# acc_ls = []\n",
    "# f1_ls = []\n",
    "# runt_ls = []\n",
    "\n",
    "# parameters = {'C':Cs}\n",
    "\n",
    "# for train_ix, test_ix in tqdm(cv.split(Xs_feat)):\n",
    "    \n",
    "#     # find the optimal hypber parameters\n",
    "#     lr = LogisticRegression(solver='saga')\n",
    "# #     clf = GridSearchCV(lr, parameters, n_jobs=1) # gridsearch cv\n",
    "#     clf = LogisticRegression(C =1.0, max_iter=5000, class_weight = 'balanced',n_jobs=1) # fixed parameter\n",
    "    \n",
    "#     scaler = preprocessing.StandardScaler().fit(Xs_feat[train_ix])\n",
    "#     X_train_scale = scaler.transform(Xs_feat[train_ix])\n",
    "    \n",
    "#     clf.fit(X_train_scale, y_cat[train_ix])\n",
    "    \n",
    "# #     print(clf.best_params_)\n",
    "# #     best_params_ls.append(clf.best_params_)\n",
    "    \n",
    "#     # predict on the test data\n",
    "#     X_test_scale = scaler.transform(Xs_feat[test_ix])\n",
    "#     y_pred, runtimes = atomic_benchmark_estimator(clf, X_test_scale, output_type= '<U3', \n",
    "#                                                   verbose=False)\n",
    "#     runt_ls.append(np.mean(runtimes))\n",
    "    \n",
    "#     acc = accuracy_score(y_cat[test_ix], y_pred)\n",
    "#     f1 = f1_score(y_cat[test_ix], y_pred, average='weighted')\n",
    "#     print('Accuracy: {:.3},\\t F1: {:.3}'.format(acc,f1))\n",
    "#     acc_ls.append(acc)\n",
    "#     f1_ls.append(f1)\n",
    "    \n",
    "# out_msg = 'Net+LR: average test acc: {:.2}, F1: {:.2}, Run-time: {:.2}ms'.format(np.mean(acc_ls), np.mean(f1_ls), np.mean(runt_ls)*1e3)\n",
    "# print(out_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2c7e6",
   "metadata": {},
   "source": [
    "## Run kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce4b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# neigh = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfedc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of neighbours: [100]\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameter kNN\n",
    "k_fold = 5\n",
    "cv = KFold(n_splits=k_fold, random_state=10, shuffle=True)\n",
    "\n",
    "# Ns=list(range(2,100,20))\n",
    "# Ns = [100]\n",
    "parameters = {'n_neighbors':Ns}\n",
    "print('list of neighbours:', Ns)\n",
    "\n",
    "Xs_arr = Vgg_Feats\n",
    "y_arr = Vgg_y_cat\n",
    "\n",
    "best_params_ls = []\n",
    "score_ls = []\n",
    "f1_ls = []\n",
    "\n",
    "for train_ix, test_ix in cv.split(Xs_arr):\n",
    "    # scale data\n",
    "    scaler = preprocessing.StandardScaler().fit(Xs_arr[train_ix])\n",
    "    X_train_scale = scaler.transform(Xs_arr[train_ix])\n",
    "    \n",
    "    # find the optimal hypber parameters\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "#     clf = GridSearchCV(neigh, parameters, n_jobs=1)\n",
    "    clf.fit(X_train_scale, y_arr[train_ix])\n",
    "#     print(clf.best_parameters)\n",
    "    \n",
    "    # predict on the test data\n",
    "    X_test_scale = scaler.transform(Xs_arr[test_ix])\n",
    "#     y_pred = clf.predict(X_test_scale)\n",
    "    y_pred, runtimes = atomic_benchmark_estimator(clf, X_test_scale, output_type= '<U3', \n",
    "                                                  verbose=False)\n",
    "    acc = accuracy_score(y_arr[test_ix], y_pred)\n",
    "    f1 = f1_score(y_arr[test_ix], y_pred, average='weighted')\n",
    "    f1_ls.append(f1)\n",
    "    print('Accuracy: {:.3},\\t F1: {:.3}'.format(acc,f1))\n",
    "    score_ls.append(acc)\n",
    "    \n",
    "print('VGG feats+kNN K-fold average test score:', np.mean(score_ls))\n",
    "print('VGG feats+kNN K-fold average test F1:', np.mean(f1_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80076188",
   "metadata": {},
   "source": [
    "## Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8392adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in Torch_Models\n",
    "# class VGGFC(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(VGGFC,self).__init__()\n",
    "#         self.num_classes = num_classes\n",
    "#         self.vggfull = models.vgg16(pretrained=True)\n",
    "#         modules=list(self.vggfull.children())[:-1] # remove the fully connected layer & adaptive averaging\n",
    "#         self.vggfeats=nn.Sequential(*modules)\n",
    "        \n",
    "#         for param in self.vggfeats.parameters():\n",
    "#             param.requires_grad_(False)\n",
    "        \n",
    "#         self._fc = nn.Linear(25088, num_classes)\n",
    "#     def forward(self, x):\n",
    "#         if len(x.shape)==4:\n",
    "#             x = torch.moveaxis(x,-1, 1)\n",
    "#         else:\n",
    "#             x = torch.moveaxis(x, -1, 0)\n",
    "#         x = self.vggfeats(x)\n",
    "# #         print(x.shape)\n",
    "#         x = x.reshape(-1,25088)\n",
    "#         x = self._fc(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1762c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNetFC(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(ResNetFC,self).__init__()\n",
    "#         self.num_classes = num_classes\n",
    "#         self.resnetfull = models.resnet50(pretrained=True)\n",
    "#         modules=list(self.resnetfull.children())[:-2] # remove the fully connected layer & adaptive averaging\n",
    "#         self.resnetfeats=nn.Sequential(*modules)\n",
    "        \n",
    "#         for param in self.resnetfeats.parameters():\n",
    "#             param.requires_grad_(False)\n",
    "        \n",
    "#         self._fc = nn.Linear(100352, num_classes)\n",
    "#     def forward(self, x):\n",
    "#         if len(x.shape)==4:\n",
    "#             x = torch.moveaxis(x,-1, 1)\n",
    "#         else:\n",
    "#             x = torch.moveaxis(x, -1, 0)\n",
    "#         x = self.resnetfeats(x)\n",
    "#         x = x.reshape(-1,100352)\n",
    "#         x = self._fc(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fdafd",
   "metadata": {},
   "source": [
    "### Kfold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c1f7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_functions import runkfoldcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f67796ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "k_folds = 5\n",
    "\n",
    "batch_size = 128 # 128\n",
    "num_classes = 7\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # 0\n",
    "momentum = 0.95\n",
    "l2reg = 1e-4\n",
    "\n",
    "Model = VGGFC(num_classes)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ece73702",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.21611\n",
      "    Loss after mini-batch   100: 0.23081\n",
      "    Loss after mini-batch   150: 0.22870\n",
      "    Loss after mini-batch   200: 0.23318\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.22025\n",
      "    Loss after mini-batch   100: 0.21542\n",
      "    Loss after mini-batch   150: 0.22238\n",
      "    Loss after mini-batch   200: 0.22282\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.21271\n",
      "    Loss after mini-batch   100: 0.21916\n",
      "    Loss after mini-batch   150: 0.21282\n",
      "    Loss after mini-batch   200: 0.21872\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.20875\n",
      "    Loss after mini-batch   100: 0.21702\n",
      "    Loss after mini-batch   150: 0.21105\n",
      "    Loss after mini-batch   200: 0.20857\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.20477\n",
      "    Loss after mini-batch   100: 0.20615\n",
      "    Loss after mini-batch   150: 0.21356\n",
      "    Loss after mini-batch   200: 0.20486\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.20678\n",
      "    Loss after mini-batch   100: 0.20490\n",
      "    Loss after mini-batch   150: 0.20323\n",
      "    Loss after mini-batch   200: 0.20357\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.20139\n",
      "    Loss after mini-batch   100: 0.20259\n",
      "    Loss after mini-batch   150: 0.20221\n",
      "    Loss after mini-batch   200: 0.21059\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.19075\n",
      "    Loss after mini-batch   100: 0.20089\n",
      "    Loss after mini-batch   150: 0.20219\n",
      "    Loss after mini-batch   200: 0.20622\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.20032\n",
      "    Loss after mini-batch   100: 0.19470\n",
      "    Loss after mini-batch   150: 0.19704\n",
      "    Loss after mini-batch   200: 0.19810\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.19359\n",
      "    Loss after mini-batch   100: 0.19233\n",
      "    Loss after mini-batch   150: 0.19559\n",
      "    Loss after mini-batch   200: 0.19293\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 0: 91.64 %\n",
      "F1 for fold 0: 0.92 \n",
      "Runtime for fold 0: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.20640\n",
      "    Loss after mini-batch   100: 0.20344\n",
      "    Loss after mini-batch   150: 0.20764\n",
      "    Loss after mini-batch   200: 0.20371\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.19874\n",
      "    Loss after mini-batch   100: 0.19279\n",
      "    Loss after mini-batch   150: 0.20458\n",
      "    Loss after mini-batch   200: 0.19378\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.19644\n",
      "    Loss after mini-batch   100: 0.20087\n",
      "    Loss after mini-batch   150: 0.19580\n",
      "    Loss after mini-batch   200: 0.19586\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.18543\n",
      "    Loss after mini-batch   100: 0.19630\n",
      "    Loss after mini-batch   150: 0.19607\n",
      "    Loss after mini-batch   200: 0.19530\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.18489\n",
      "    Loss after mini-batch   100: 0.19480\n",
      "    Loss after mini-batch   150: 0.19048\n",
      "    Loss after mini-batch   200: 0.18585\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.18172\n",
      "    Loss after mini-batch   100: 0.18546\n",
      "    Loss after mini-batch   150: 0.19167\n",
      "    Loss after mini-batch   200: 0.18302\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.18040\n",
      "    Loss after mini-batch   100: 0.18397\n",
      "    Loss after mini-batch   150: 0.17943\n",
      "    Loss after mini-batch   200: 0.19092\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.18657\n",
      "    Loss after mini-batch   100: 0.18231\n",
      "    Loss after mini-batch   150: 0.17525\n",
      "    Loss after mini-batch   200: 0.18513\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.18527\n",
      "    Loss after mini-batch   100: 0.17363\n",
      "    Loss after mini-batch   150: 0.17686\n",
      "    Loss after mini-batch   200: 0.18654\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.17766\n",
      "    Loss after mini-batch   100: 0.17687\n",
      "    Loss after mini-batch   150: 0.18324\n",
      "    Loss after mini-batch   200: 0.17613\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 1: 92.87 %\n",
      "F1 for fold 1: 0.93 \n",
      "Runtime for fold 1: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.18953\n",
      "    Loss after mini-batch   100: 0.18278\n",
      "    Loss after mini-batch   150: 0.19074\n",
      "    Loss after mini-batch   200: 0.18323\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.18251\n",
      "    Loss after mini-batch   100: 0.17989\n",
      "    Loss after mini-batch   150: 0.18577\n",
      "    Loss after mini-batch   200: 0.18109\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.17309\n",
      "    Loss after mini-batch   100: 0.18050\n",
      "    Loss after mini-batch   150: 0.18379\n",
      "    Loss after mini-batch   200: 0.17434\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.17085\n",
      "    Loss after mini-batch   100: 0.17058\n",
      "    Loss after mini-batch   150: 0.18564\n",
      "    Loss after mini-batch   200: 0.17604\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.16926\n",
      "    Loss after mini-batch   100: 0.17229\n",
      "    Loss after mini-batch   150: 0.17867\n",
      "    Loss after mini-batch   200: 0.16974\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.16943\n",
      "    Loss after mini-batch   100: 0.17833\n",
      "    Loss after mini-batch   150: 0.17062\n",
      "    Loss after mini-batch   200: 0.17068\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.16834\n",
      "    Loss after mini-batch   100: 0.16996\n",
      "    Loss after mini-batch   150: 0.16754\n",
      "    Loss after mini-batch   200: 0.17007\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.16992\n",
      "    Loss after mini-batch   100: 0.15968\n",
      "    Loss after mini-batch   150: 0.16076\n",
      "    Loss after mini-batch   200: 0.16875\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.15917\n",
      "    Loss after mini-batch   100: 0.16284\n",
      "    Loss after mini-batch   150: 0.16923\n",
      "    Loss after mini-batch   200: 0.16468\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.16372\n",
      "    Loss after mini-batch   100: 0.16020\n",
      "    Loss after mini-batch   150: 0.15630\n",
      "    Loss after mini-batch   200: 0.16888\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 2: 94.09 %\n",
      "F1 for fold 2: 0.94 \n",
      "Runtime for fold 2: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.17259\n",
      "    Loss after mini-batch   100: 0.16599\n",
      "    Loss after mini-batch   150: 0.17562\n",
      "    Loss after mini-batch   200: 0.16449\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.16340\n",
      "    Loss after mini-batch   100: 0.16551\n",
      "    Loss after mini-batch   150: 0.16449\n",
      "    Loss after mini-batch   200: 0.16293\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.15863\n",
      "    Loss after mini-batch   100: 0.16278\n",
      "    Loss after mini-batch   150: 0.16199\n",
      "    Loss after mini-batch   200: 0.16354\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.14981\n",
      "    Loss after mini-batch   100: 0.16087\n",
      "    Loss after mini-batch   150: 0.16499\n",
      "    Loss after mini-batch   200: 0.16111\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.15910\n",
      "    Loss after mini-batch   100: 0.15280\n",
      "    Loss after mini-batch   150: 0.15817\n",
      "    Loss after mini-batch   200: 0.15396\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.15089\n",
      "    Loss after mini-batch   100: 0.15658\n",
      "    Loss after mini-batch   150: 0.14915\n",
      "    Loss after mini-batch   200: 0.16400\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.14532\n",
      "    Loss after mini-batch   100: 0.15889\n",
      "    Loss after mini-batch   150: 0.15310\n",
      "    Loss after mini-batch   200: 0.15493\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.15123\n",
      "    Loss after mini-batch   100: 0.15333\n",
      "    Loss after mini-batch   150: 0.15368\n",
      "    Loss after mini-batch   200: 0.15296\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.15109\n",
      "    Loss after mini-batch   100: 0.14296\n",
      "    Loss after mini-batch   150: 0.15182\n",
      "    Loss after mini-batch   200: 0.14831\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.14473\n",
      "    Loss after mini-batch   100: 0.14675\n",
      "    Loss after mini-batch   150: 0.14471\n",
      "    Loss after mini-batch   200: 0.15003\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 3: 94.61 %\n",
      "F1 for fold 3: 0.95 \n",
      "Runtime for fold 3: 0.0057 s\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.15476\n",
      "    Loss after mini-batch   100: 0.15585\n",
      "    Loss after mini-batch   150: 0.15589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss after mini-batch   200: 0.15825\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.14772\n",
      "    Loss after mini-batch   100: 0.15783\n",
      "    Loss after mini-batch   150: 0.15562\n",
      "    Loss after mini-batch   200: 0.15370\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.15013\n",
      "    Loss after mini-batch   100: 0.15044\n",
      "    Loss after mini-batch   150: 0.14894\n",
      "    Loss after mini-batch   200: 0.15829\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.14647\n",
      "    Loss after mini-batch   100: 0.14365\n",
      "    Loss after mini-batch   150: 0.15970\n",
      "    Loss after mini-batch   200: 0.15232\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.15028\n",
      "    Loss after mini-batch   100: 0.14706\n",
      "    Loss after mini-batch   150: 0.14865\n",
      "    Loss after mini-batch   200: 0.14988\n",
      "Starting epoch 6\n",
      "    Loss after mini-batch    50: 0.14911\n",
      "    Loss after mini-batch   100: 0.14035\n",
      "    Loss after mini-batch   150: 0.14213\n",
      "    Loss after mini-batch   200: 0.14785\n",
      "Starting epoch 7\n",
      "    Loss after mini-batch    50: 0.14468\n",
      "    Loss after mini-batch   100: 0.14567\n",
      "    Loss after mini-batch   150: 0.14202\n",
      "    Loss after mini-batch   200: 0.14324\n",
      "Starting epoch 8\n",
      "    Loss after mini-batch    50: 0.13728\n",
      "    Loss after mini-batch   100: 0.14822\n",
      "    Loss after mini-batch   150: 0.14315\n",
      "    Loss after mini-batch   200: 0.14471\n",
      "Starting epoch 9\n",
      "    Loss after mini-batch    50: 0.12977\n",
      "    Loss after mini-batch   100: 0.14382\n",
      "    Loss after mini-batch   150: 0.14104\n",
      "    Loss after mini-batch   200: 0.14321\n",
      "Starting epoch 10\n",
      "    Loss after mini-batch    50: 0.13555\n",
      "    Loss after mini-batch   100: 0.14104\n",
      "    Loss after mini-batch   150: 0.14148\n",
      "    Loss after mini-batch   200: 0.14219\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 4: 95.66 %\n",
      "F1 for fold 4: 0.96 \n",
      "Runtime for fold 4: 0.0057 s\n",
      "--------------------------------\n",
      "K-FOLD CROSS VALIDATION RESULTS FOR 5 FOLDS\n",
      "--------------------------------\n",
      "Fold 0: 91.63673678809646 %\n",
      "Fold 1: 92.86813750641355 %\n",
      "Fold 2: 94.08671113391482 %\n",
      "Fold 3: 94.6119307248236 %\n",
      "Fold 4: 95.66388710711995 %\n",
      "Average Accuracy: 93.77348065207367 %\n",
      "Average F1: 0.9379559245843282\n",
      "Average Runtime: 0.005712414296929129 s\n"
     ]
    }
   ],
   "source": [
    "trainedModel, res_acc, res_f1, res_runtime = runkfoldcv(Model, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f3dcfd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGFC(\n",
       "  (vggfull): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (vggfeats): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (_fc): Linear(in_features=25088, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26916480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93.77348065207367, 0.9379559245843282, 0.005712414296929129)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_acc, res_f1, res_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cec40d",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8ef036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as: VGGFC_PSD_1024_20\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model_type = 'VGGFC'\n",
    "model_name = model_type+'_'+str(feat_name)+'_'+str(n_per_seg)+'_'+str(seg_len)\n",
    "model_path = '../saved_models/'\n",
    "trainedModel = trainedModel.cpu()\n",
    "torch.save(trainedModel, model_path+model_name)\n",
    "print('Model saved as:', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195f509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test reload\n",
    "# m2 = torch.load(model_path+model_name)\n",
    "# in2 = dataset.__getitem__(0)[0]\n",
    "# out = m2(in2)\n",
    "# _,pred = torch.max(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a2438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
