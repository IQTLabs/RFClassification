{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4e062b",
   "metadata": {},
   "source": [
    "## Deep Learning Approaches for RF-based detection & classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df87784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "# import the torch packages\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import custom functions\n",
    "from helper_functions import *\n",
    "from latency_helpers import *\n",
    "from loading_functions import *\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4835a",
   "metadata": {},
   "source": [
    "### Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7710a0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [01:36<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "feat_folder = '../Features/'\n",
    "feat_name = 'SPEC'\n",
    "seg_len = 20\n",
    "# datestr = '2022-07-05'\n",
    "n_per_seg = 256\n",
    "interferences = ['CLEAN']\n",
    "dataset = load_dronedetect_data(feat_folder, feat_name, seg_len, n_per_seg, interferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b732f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 9487\n",
      "shape of each item torch.Size([1, 129, 4687])\n"
     ]
    }
   ],
   "source": [
    "print('dataset size', len(dataset))\n",
    "print('shape of each item', dataset.__getitem__(10)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ceef8f",
   "metadata": {},
   "source": [
    "## Transfer learning from Resnet50 & Apply Logistic Regression (Swinney paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47b6d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained resnet feature and just keep up to the last layer\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "modules=list(resnet50.children())[:-1]\n",
    "# resnet50=nn.Sequential(*modules)\n",
    "for p in resnet50.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "440b23cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test resnet\n",
    "input = torch.randn(1,1,30,300)\n",
    "inputr = input.repeat(1,3,1,1)\n",
    "resnet50(inputr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5a73489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_feats = []\n",
    "resnet_y = []\n",
    "for n in range(len(dataset)):\n",
    "    d = dataset.__getitem__(n)\n",
    "    inarr = d[0]\n",
    "    inputr = inarr.repeat(1,3,1,1)  # repeat to have 3 channels of the same info\n",
    "    out = resnet50(inputr)\n",
    "    resnet_feats.append(np.array(out))\n",
    "    resnet_y.append(np.array(d[1]))\n",
    "\n",
    "resnet_feats = np.array(resnet_feats)\n",
    "resnet_y = np.array(resnet_y)\n",
    "\n",
    "# flatten the middle dimension\n",
    "resnet_feats = resnet_feats.reshape(resnet_feats.shape[0], resnet_feats.shape[-1])\n",
    "# invert labels back to categorical\n",
    "resnet_y_cat = dataset.le.inverse_transform(resnet_y.astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697ba2b",
   "metadata": {},
   "source": [
    "## Transfer learning from VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8031e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kzhou/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "vgg19 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)\n",
    "modules=list(vgg19.children())[:-1]\n",
    "# resnet50=nn.Sequential(*modules)\n",
    "for p in vgg19.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa2ea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9487/9487 [1:54:38<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "vgg_feats = []\n",
    "vgg_y = []\n",
    "for n in tqdm(range(len(dataset))):\n",
    "    d = dataset.__getitem__(n)\n",
    "    inarr = d[0]\n",
    "    inputr = inarr.repeat(1,3,1,1)  # repeat to have 3 channels of the same info\n",
    "    out = vgg19(inputr)\n",
    "    vgg_feats.append(np.array(out))\n",
    "    vgg_y.append(np.array(d[1]))\n",
    "\n",
    "vgg_feats = np.array(vgg_feats)\n",
    "vgg_y = np.array(vgg_y)\n",
    "\n",
    "# flatten the middle dimension\n",
    "vgg_feats = vgg_feats.reshape(vgg_feats.shape[0], vgg_feats.shape[-1])\n",
    "# invert labels back to categorical\n",
    "vgg_y_cat = dataset.le.inverse_transform(vgg_y.astype(np.int64))\n",
    "\n",
    "### TO DO: Save these features to be easily loaded [ this took almost 2 hours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86506b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_lr = vgg_feats # which features to use for logit reg\n",
    "y_cat = vgg_y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eea904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# split data into K-fold\n",
    "k_fold = 5\n",
    "cv = KFold(n_splits=k_fold, random_state=1, shuffle=True)\n",
    "\n",
    "# model parameters\n",
    "Cs=list(map(lambda x:pow(2,x),range(-2,10,2)))\n",
    "\n",
    "best_params_ls = []\n",
    "acc_ls = []\n",
    "f1_ls = []\n",
    "runt_ls = []\n",
    "\n",
    "parameters = {'C':Cs}\n",
    "\n",
    "for train_ix, test_ix in tqdm(cv.split(feats_lr)):\n",
    "    \n",
    "    # find the optimal hypber parameters\n",
    "    lr = LogisticRegression(max_iter=10000)\n",
    "    clf = GridSearchCV(lr, parameters, n_jobs=1)\n",
    "    clf.fit(feats_lr[train_ix], y_cat[train_ix])\n",
    "    \n",
    "    print(clf.best_params_)\n",
    "    best_params_ls.append(clf.best_params_)\n",
    "    \n",
    "    # predict on the test data\n",
    "    y_pred, runtimes = atomic_benchmark_estimator(clf, feats_lr[test_ix], verbose=False)\n",
    "    runt_ls.append(np.mean(runtimes))\n",
    "    \n",
    "    acc = accuracy_score(y_arr[test_ix], y_pred)\n",
    "    f1 = f1_score(y_arr[test_ix], y_pred, average='weighted')\n",
    "    print('Accuracy: {:.3},\\t F1: {:.3}'.format(acc,f1))\n",
    "    acc_ls.append(acc)\n",
    "    f1_ls.append(f1)\n",
    "    \n",
    "out_msg = feat_name+': ResNet+LR average test acc: {:.2}, F1: {:.2}, Run-time: {:.2}ms'.format(np.mean(acc_ls), np.mean(f1_ls), np.mean(runt_ls)*1e3)\n",
    "print(out_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d56ed",
   "metadata": {},
   "source": [
    "## 3. Apply resnet & a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define network\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
    "# model.classifier = nn.Linear(model.classifier[0].in_features, num_classes)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfdbf77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetFC(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResnetFC,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        for param in self.resnet.parameters():\n",
    "            self.resnet.requires_grad_(False)\n",
    "        \n",
    "        self._fc = nn.Linear(1000, num_classes)\n",
    "    def forward(self, x):\n",
    "#         batch_size ,_,_ =x.shape\n",
    "        \n",
    "        # replicate the image to have 3 channels\n",
    "        x = x.repeat(1,3,1,1)\n",
    "        x = self.resnet(x)\n",
    "        x = self._fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "793ea710",
   "metadata": {},
   "outputs": [],
   "source": [
    "fctest = ResnetFC(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2aa0925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0363, -1.0948,  0.0186,  0.1136, -0.1828,  0.1280, -0.2705]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fctest.forward(dataset.__getitem__(10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ce6d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a dataset out of resnet features\n",
    "# class ResNetFeatData(Dataset): ## NUMBERICAL DATA\n",
    "#     def __init__(self, Xarr, yarr):\n",
    "#         self.Xarr = Xarr\n",
    "#         test_list=[]\n",
    "#         self.le = preprocessing.LabelEncoder()\n",
    "#         self.le.fit(yarr.flatten())\n",
    "#         self.yarr = le.transform(yarr.flatten())\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.yarr)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         # all data must be in float and tensor format\n",
    "#         X = torch.tensor((self.Xarr[index]))\n",
    "#         X = X.unsqueeze(0)\n",
    "#         y = torch.tensor(float(self.yarr[index]))\n",
    "#         return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a939ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resdataset = ResNetFeatData(resnet_feats, y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc22895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Network for fully connected layer\n",
    "# class FCNet(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(FCNet, self).__init__()\n",
    "#         self.fc_layer = nn.Linear(1000, num_classes)\n",
    "    \n",
    "#     # Progresses data across layers    \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc_layer(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11227657",
   "metadata": {},
   "source": [
    "### development code - function for cross validation for any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2508ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runkfoldcv(model, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg):\n",
    "    num_classes = model.num_classes\n",
    "    \n",
    "    # For fold results\n",
    "    results = {}\n",
    "    runtimes = np.zeros(k_folds)\n",
    "    f1s = np.zeros(k_folds)\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    # Start print\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "        # Print\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        # Sample elements randomly from a given list of ids, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "        # Define data loaders for training and testing data in this fold\n",
    "        trainloader = torch.utils.data.DataLoader(\n",
    "                          dataset, \n",
    "                          batch_size=batch_size, sampler=train_subsampler)\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "                          dataset,\n",
    "                          batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "        # Init the neural network\n",
    "        model = model.to(device)\n",
    "    #     network.apply(reset_weights)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Initialize optimizer\n",
    "    #     optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2reg, momentum = momentum)  \n",
    "\n",
    "        # Run the training loop for defined number of epochs\n",
    "        for epoch in range(0, num_epochs):\n",
    "            # Print epoch\n",
    "            print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "            # Set current loss value\n",
    "            current_loss = 0.0\n",
    "\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(trainloader):\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "                inputs = inputs.float()\n",
    "#                 inputs = torch.squeeze(inputs, 1)\n",
    "                targets= targets.type(torch.long)\n",
    "\n",
    "                # Move tensors to configured device\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                \n",
    "\n",
    "                # Perform forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss            \n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Perform backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform optimization\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "                current_loss += loss.item()\n",
    "                if i % 50 == 49:\n",
    "                    print('    Loss after mini-batch %5d: %.5f' %\n",
    "                          (i + 1, current_loss / 50))\n",
    "                    current_loss = 0.0\n",
    "    #         print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "        # Process is complete.\n",
    "    #     print('Training process has finished. Saving trained model.')\n",
    "\n",
    "        # Print about testing\n",
    "        print('Starting testing')\n",
    "        print('----------------')\n",
    "\n",
    "        # Saving the model\n",
    "    #     save_path = f'./model-fold-{fold}.pth'\n",
    "    #     torch.save(network.state_dict(), save_path)\n",
    "\n",
    "        # Evaluation for this fold\n",
    "        correct, total = 0, 0\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            runtimes_thisfold = []\n",
    "            f1s_thisfold = []\n",
    "            # Iterate over the test data and generate predictions\n",
    "            for i, data in enumerate(testloader, 0):\n",
    "                # Get inputs\n",
    "                inputs, targets = data\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Generate outputs\n",
    "                n_instances = len(inputs)\n",
    "                ys = torch.empty(n_instances)\n",
    "                ys = ys.to(device)\n",
    "\n",
    "                for i in range(n_instances):\n",
    "                    instance = inputs[i]\n",
    "                    instance = instance.float()\n",
    "                    start = time.time()\n",
    "                    starter.record()\n",
    "                    yi = model(instance)\n",
    "                    _,pred = torch.max(yi,1)\n",
    "                    ender.record()\n",
    "\n",
    "                    torch.cuda.synchronize()\n",
    "                    curr_time = starter.elapsed_time(ender) #miliseconds\n",
    "\n",
    "                    runtimes_thisfold.append(curr_time*1e-3)\n",
    "                    ys[i] = pred\n",
    "\n",
    "\n",
    "                # Set total and correct\n",
    "                total += targets.size(0)\n",
    "                correct += (ys == targets).sum().item()\n",
    "                f1i = f1_score(ys.cpu().numpy(), targets.cpu().numpy())\n",
    "                f1s_thisfold.append(f1i)\n",
    "\n",
    "            mean_runtime = np.mean(np.array(runtimes_thisfold))\n",
    "            mean_f1 = np.mean(np.array(f1s_thisfold))\n",
    "\n",
    "        # Summarize and print results\n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "        runtimes[fold] = mean_runtime\n",
    "        f1s[fold] = mean_f1\n",
    "        print('Accuracy for fold %d: %.2f %%' % (fold, 100.0 * correct / total))\n",
    "        print('F1 for fold %d: %.2f ' % (fold, mean_f1))\n",
    "        print('Runtime for fold %d: %.4f s' % (fold, mean_runtime))\n",
    "        print('--------------------------------')\n",
    "\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "    mean_acc = sum/len(results.items())\n",
    "    mean_f1s = np.mean(f1s)\n",
    "    mean_runtime = np.mean(runtimes)\n",
    "    print(f'Average Accuracy: {mean_acc} %')\n",
    "    print(f'Average F1: {mean_f1s}')\n",
    "    print(f'Average Runtime: {mean_runtime} s')\n",
    "    \n",
    "    return avg_acc, mean_f1s, mean_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options\n",
    "k_folds = 2\n",
    "\n",
    "batch_size = 8 # 128\n",
    "num_classes = 7\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1 # 0\n",
    "momentum = 0.95\n",
    "l2reg = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a21bbcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.65 GiB (GPU 0; 23.65 GiB total capacity; 11.67 GiB already allocated; 2.77 GiB free; 11.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrunkfoldcv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfctest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2reg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mrunkfoldcv\u001b[0;34m(model, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg)\u001b[0m\n\u001b[1;32m     61\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Perform forward pass\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Compute loss            \u001b[39;00m\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mResnetFC.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#         batch_size ,_,_ =x.shape\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# replicate the image to have 3 channels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fc(x)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/torchvision/models/resnet.py:283\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/torchvision/models/resnet.py:267\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m--> 267\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.2/envs/main/lib/python3.8/site-packages/torch/nn/functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2419\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.65 GiB (GPU 0; 23.65 GiB total capacity; 11.67 GiB already allocated; 2.77 GiB free; 11.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "runkfoldcv(fctest, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89f4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
