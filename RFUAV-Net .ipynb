{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d254ebea",
   "metadata": {},
   "source": [
    "## Implementation of RFUAV-net\n",
    "efficient CNN method - 1D convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3259ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from helper_functions import *\n",
    "from loading_functions import *\n",
    "\n",
    "import time\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "# from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147ca17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nn_functions' from '/home/kzhou/main/RFClassification/nn_functions.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload functions & modules\n",
    "import importlib\n",
    "import nn_functions\n",
    "importlib.reload(nn_functions)\n",
    "# from loading_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56271efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff 5 file name: 00000L_13.csv\n"
     ]
    }
   ],
   "source": [
    "## Import data -  Drone RF\n",
    "main_folder = '/home/kzhou/Data/DroneRF/'\n",
    "t_seg = 0.25 #ms\n",
    "Xs_arr, ys_arr, y4s_arr, y10s_arr = load_dronerf_raw(main_folder, t_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44423cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for nans in the data\n",
    "# tfall = np.isnan(y10s_arr)\n",
    "# tfall.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9047d",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45b4212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DroneData(Xs_arr, y10s_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f03644a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa173e26370>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dataset.__getitem__(40)[0][1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086be216",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c407a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFUAVNet(nn.Module):\n",
    "    #  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(RFUAVNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.dense = nn.Linear(320, num_classes)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.smax = nn.Softmax(dim=1)\n",
    "        \n",
    "        # for r unit\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=5, stride=5)\n",
    "        self.norm1 = nn.BatchNorm1d(num_features=64)\n",
    "        self.elu1 = nn.ELU(alpha=1.0, inplace=False)\n",
    "        \n",
    "        # setup for components of the gunit\n",
    "        self.groupconvlist = []\n",
    "        self.norm2list = []\n",
    "        self.elu2list = []\n",
    "        for i in range(4):\n",
    "            self.groupconvlist.append( nn.Conv1d( \n",
    "                  in_channels=64,\n",
    "                  out_channels=64,\n",
    "                  kernel_size=3,\n",
    "                  stride = 2,\n",
    "                  groups=8,\n",
    "    #               bias=False,\n",
    "                  dtype=torch.float32\n",
    "                ))\n",
    "            self.norm2list.append(nn.BatchNorm1d(num_features=64))\n",
    "            self.elu2list.append(nn.ELU(alpha=1.0, inplace=False))\n",
    "        self.groupconv = nn.ModuleList(self.groupconvlist)\n",
    "        self.norm2 = nn.ModuleList(self.norm2list)\n",
    "        self.elu2 = nn.ModuleList(self.elu2list)\n",
    "        \n",
    "        # multi-gap implementation\n",
    "        self.avgpool1000 = nn.AvgPool1d(kernel_size=1000)\n",
    "        self.avgpool500 = nn.AvgPool1d(kernel_size=500)\n",
    "        self.avgpool250 = nn.AvgPool1d(kernel_size=250)\n",
    "        self.avgpool125 = nn.AvgPool1d(kernel_size=125)\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        # runit first\n",
    "        x1 = self.runit(x)\n",
    "        xg1 = self.gunit(F.pad(x1, (1,0)), 0) \n",
    "        x2 = self.pool(x1)\n",
    "        x3 = xg1+x2\n",
    "        \n",
    "        # series of gunits\n",
    "        xg2 = self.gunit(F.pad(x3, (1,0)), 1)\n",
    "        x4 = self.pool(x3)\n",
    "        x5 = xg2+x4\n",
    "        \n",
    "        xg3 = self.gunit(F.pad(x5, (1,0)), 2)\n",
    "        x6 = self.pool(x5)\n",
    "        x7 = x6+xg3\n",
    "        \n",
    "        xg4 = self.gunit(F.pad(x7, (1,0)), 3)\n",
    "        x8 = self.pool(x7)\n",
    "        x_togap = x8+xg4\n",
    "        \n",
    "        \n",
    "        # gap and multi-gap\n",
    "        f_gap_1 = self.avgpool1000(xg1)\n",
    "        f_gap_2 = self.avgpool500(xg2)\n",
    "        f_gap_3 = self.avgpool250(xg3)\n",
    "        f_gap_4 = self.avgpool125(xg4)\n",
    "        \n",
    "        f_multigap = torch.cat((f_gap_1,f_gap_2, f_gap_3, f_gap_4), 1)\n",
    "        \n",
    "        f_gap_add = self.avgpool125(x_togap)\n",
    "    \n",
    "        f_final = torch.cat((f_multigap, f_gap_add),1)\n",
    "        f_flat = f_final.flatten(start_dim=1)\n",
    "    \n",
    "        out = self.dense(f_flat)\n",
    "#         out = self.smax(f_fc)\n",
    "        # fc_layer\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def runit(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.elu1(x)\n",
    "        return x\n",
    "        \n",
    "    def gunit(self, x, n):\n",
    "        # group convolution layer 8 by 8\n",
    "        # norm\n",
    "        # elu\n",
    "        # n indicates which gunit\n",
    "        x = self.groupconv[n](x) \n",
    "        x = self.norm2[n](x)\n",
    "        x = self.elu2[n](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a2e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10000])\n",
      "torch.Size([1, 2, 10000])\n"
     ]
    }
   ],
   "source": [
    "## Test network\n",
    "input1 = dataset.__getitem__(40)[0]\n",
    "# input1= input1.type(torch.float)\n",
    "print(input1.shape)\n",
    "input1 = torch.unsqueeze(input1, 0)\n",
    "# input = input.reshape(1, 2, 10000)\n",
    "# input1 = torch.rand(128, 2, 10000)\n",
    "\n",
    "print(input1.shape)\n",
    "\n",
    "# input_1d = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype = torch.float)\n",
    "\n",
    "net = RFUAVNet(3)\n",
    "out = net(input1)\n",
    "\n",
    "# print(out.shape)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e55c3b",
   "metadata": {},
   "source": [
    "## Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1be75eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_functions import runkfoldcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8efa8a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyperparameters\n",
    "num_classes = 10\n",
    "batch_size = 128 # 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5 # 0\n",
    "momentum = 0.95\n",
    "l2reg = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9293e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RFUAVNet(num_classes)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49948062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 2.41542\n",
      "    Loss after mini-batch   100: 2.26214\n",
      "    Loss after mini-batch   150: 2.24467\n",
      "    Loss after mini-batch   200: 2.22493\n",
      "    Loss after mini-batch   250: 2.21494\n",
      "    Loss after mini-batch   300: 2.19337\n",
      "    Loss after mini-batch   350: 2.18426\n",
      "    Loss after mini-batch   400: 2.16575\n",
      "    Loss after mini-batch   450: 2.15854\n",
      "    Loss after mini-batch   500: 2.12686\n",
      "    Loss after mini-batch   550: 2.09452\n",
      "    Loss after mini-batch   600: 2.05638\n",
      "    Loss after mini-batch   650: 2.03202\n",
      "    Loss after mini-batch   700: 1.99310\n",
      "    Loss after mini-batch   750: 1.93961\n",
      "    Loss after mini-batch   800: 1.90332\n",
      "    Loss after mini-batch   850: 1.84967\n",
      "    Loss after mini-batch   900: 1.80608\n",
      "    Loss after mini-batch   950: 1.74293\n",
      "    Loss after mini-batch  1000: 1.71577\n",
      "    Loss after mini-batch  1050: 1.64929\n",
      "    Loss after mini-batch  1100: 1.63925\n",
      "    Loss after mini-batch  1150: 1.59304\n",
      "    Loss after mini-batch  1200: 1.54583\n",
      "    Loss after mini-batch  1250: 1.49631\n",
      "    Loss after mini-batch  1300: 1.46905\n",
      "    Loss after mini-batch  1350: 1.43385\n",
      "    Loss after mini-batch  1400: 1.40802\n",
      "    Loss after mini-batch  1450: 1.36928\n",
      "    Loss after mini-batch  1500: 1.33482\n",
      "    Loss after mini-batch  1550: 1.31662\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 1.27722\n",
      "    Loss after mini-batch   100: 1.23731\n",
      "    Loss after mini-batch   150: 1.19672\n",
      "    Loss after mini-batch   200: 1.17935\n",
      "    Loss after mini-batch   250: 1.14409\n",
      "    Loss after mini-batch   300: 1.11851\n",
      "    Loss after mini-batch   350: 1.09698\n",
      "    Loss after mini-batch   400: 1.05470\n",
      "    Loss after mini-batch   450: 1.04162\n",
      "    Loss after mini-batch   500: 1.01836\n",
      "    Loss after mini-batch   550: 0.98286\n",
      "    Loss after mini-batch   600: 0.96902\n",
      "    Loss after mini-batch   650: 0.93103\n",
      "    Loss after mini-batch   700: 0.92773\n",
      "    Loss after mini-batch   750: 0.90785\n",
      "    Loss after mini-batch   800: 0.89144\n",
      "    Loss after mini-batch   850: 0.88170\n",
      "    Loss after mini-batch   900: 0.87128\n",
      "    Loss after mini-batch   950: 0.86303\n",
      "    Loss after mini-batch  1000: 0.83822\n",
      "    Loss after mini-batch  1050: 0.82566\n",
      "    Loss after mini-batch  1100: 0.80923\n",
      "    Loss after mini-batch  1150: 0.79898\n",
      "    Loss after mini-batch  1200: 0.79551\n",
      "    Loss after mini-batch  1250: 0.79475\n",
      "    Loss after mini-batch  1300: 0.79836\n",
      "    Loss after mini-batch  1350: 0.76782\n",
      "    Loss after mini-batch  1400: 0.77638\n",
      "    Loss after mini-batch  1450: 0.75200\n",
      "    Loss after mini-batch  1500: 0.74115\n",
      "    Loss after mini-batch  1550: 0.75564\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.74075\n",
      "    Loss after mini-batch   100: 0.73781\n",
      "    Loss after mini-batch   150: 0.73497\n",
      "    Loss after mini-batch   200: 0.72971\n",
      "    Loss after mini-batch   250: 0.70561\n",
      "    Loss after mini-batch   300: 0.69713\n",
      "    Loss after mini-batch   350: 0.70088\n",
      "    Loss after mini-batch   400: 0.68843\n",
      "    Loss after mini-batch   450: 0.70088\n",
      "    Loss after mini-batch   500: 0.69302\n",
      "    Loss after mini-batch   550: 0.66308\n",
      "    Loss after mini-batch   600: 0.68232\n",
      "    Loss after mini-batch   650: 0.66068\n",
      "    Loss after mini-batch   700: 0.67495\n",
      "    Loss after mini-batch   750: 0.68026\n",
      "    Loss after mini-batch   800: 0.65873\n",
      "    Loss after mini-batch   850: 0.64980\n",
      "    Loss after mini-batch   900: 0.65081\n",
      "    Loss after mini-batch   950: 0.64944\n",
      "    Loss after mini-batch  1000: 0.65508\n",
      "    Loss after mini-batch  1050: 0.65015\n",
      "    Loss after mini-batch  1100: 0.63710\n",
      "    Loss after mini-batch  1150: 0.63241\n",
      "    Loss after mini-batch  1200: 0.63111\n",
      "    Loss after mini-batch  1250: 0.64868\n",
      "    Loss after mini-batch  1300: 0.63512\n",
      "    Loss after mini-batch  1350: 0.63693\n",
      "    Loss after mini-batch  1400: 0.62387\n",
      "    Loss after mini-batch  1450: 0.60578\n",
      "    Loss after mini-batch  1500: 0.63771\n",
      "    Loss after mini-batch  1550: 0.59939\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.61265\n",
      "    Loss after mini-batch   100: 0.61353\n",
      "    Loss after mini-batch   150: 0.59888\n",
      "    Loss after mini-batch   200: 0.62294\n",
      "    Loss after mini-batch   250: 0.59596\n",
      "    Loss after mini-batch   300: 0.59873\n",
      "    Loss after mini-batch   350: 0.60133\n",
      "    Loss after mini-batch   400: 0.59816\n",
      "    Loss after mini-batch   450: 0.59121\n",
      "    Loss after mini-batch   500: 0.60364\n",
      "    Loss after mini-batch   550: 0.59486\n",
      "    Loss after mini-batch   600: 0.58781\n",
      "    Loss after mini-batch   650: 0.58791\n",
      "    Loss after mini-batch   700: 0.57284\n",
      "    Loss after mini-batch   750: 0.56290\n",
      "    Loss after mini-batch   800: 0.59444\n",
      "    Loss after mini-batch   850: 0.57887\n",
      "    Loss after mini-batch   900: 0.59472\n",
      "    Loss after mini-batch   950: 0.58462\n",
      "    Loss after mini-batch  1000: 0.56664\n",
      "    Loss after mini-batch  1050: 0.58741\n",
      "    Loss after mini-batch  1100: 0.56015\n",
      "    Loss after mini-batch  1150: 0.56982\n",
      "    Loss after mini-batch  1200: 0.57928\n",
      "    Loss after mini-batch  1250: 0.57326\n",
      "    Loss after mini-batch  1300: 0.56868\n",
      "    Loss after mini-batch  1350: 0.57262\n",
      "    Loss after mini-batch  1400: 0.56736\n",
      "    Loss after mini-batch  1450: 0.55814\n",
      "    Loss after mini-batch  1500: 0.55860\n",
      "    Loss after mini-batch  1550: 0.56563\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.56983\n",
      "    Loss after mini-batch   100: 0.54684\n",
      "    Loss after mini-batch   150: 0.55969\n",
      "    Loss after mini-batch   200: 0.54840\n",
      "    Loss after mini-batch   250: 0.54473\n",
      "    Loss after mini-batch   300: 0.56198\n",
      "    Loss after mini-batch   350: 0.54265\n",
      "    Loss after mini-batch   400: 0.55217\n",
      "    Loss after mini-batch   450: 0.53069\n",
      "    Loss after mini-batch   500: 0.55590\n",
      "    Loss after mini-batch   550: 0.53970\n",
      "    Loss after mini-batch   600: 0.54054\n",
      "    Loss after mini-batch   650: 0.54307\n",
      "    Loss after mini-batch   700: 0.55972\n",
      "    Loss after mini-batch   750: 0.54694\n",
      "    Loss after mini-batch   800: 0.52333\n",
      "    Loss after mini-batch   850: 0.55031\n",
      "    Loss after mini-batch   900: 0.53622\n",
      "    Loss after mini-batch   950: 0.53746\n",
      "    Loss after mini-batch  1000: 0.53134\n",
      "    Loss after mini-batch  1050: 0.53392\n",
      "    Loss after mini-batch  1100: 0.54433\n",
      "    Loss after mini-batch  1150: 0.53395\n",
      "    Loss after mini-batch  1200: 0.53280\n",
      "    Loss after mini-batch  1250: 0.54519\n",
      "    Loss after mini-batch  1300: 0.53349\n",
      "    Loss after mini-batch  1350: 0.53928\n",
      "    Loss after mini-batch  1400: 0.52854\n",
      "    Loss after mini-batch  1450: 0.53694\n",
      "    Loss after mini-batch  1500: 0.52150\n",
      "    Loss after mini-batch  1550: 0.51965\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 0: 79.32 %\n",
      "F1 for fold 0: 0.79 \n",
      "Runtime for fold 0: 0.0011 s\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.50312\n",
      "    Loss after mini-batch   100: 0.52097\n",
      "    Loss after mini-batch   150: 0.51247\n",
      "    Loss after mini-batch   200: 0.51405\n",
      "    Loss after mini-batch   250: 0.50795\n",
      "    Loss after mini-batch   300: 0.49783\n",
      "    Loss after mini-batch   350: 0.51364\n",
      "    Loss after mini-batch   400: 0.48733\n",
      "    Loss after mini-batch   450: 0.49821\n",
      "    Loss after mini-batch   500: 0.49238\n",
      "    Loss after mini-batch   550: 0.50393\n",
      "    Loss after mini-batch   600: 0.49066\n",
      "    Loss after mini-batch   650: 0.49480\n",
      "    Loss after mini-batch   700: 0.46862\n",
      "    Loss after mini-batch   750: 0.49142\n",
      "    Loss after mini-batch   800: 0.50334\n",
      "    Loss after mini-batch   850: 0.49126\n",
      "    Loss after mini-batch   900: 0.48309\n",
      "    Loss after mini-batch   950: 0.49445\n",
      "    Loss after mini-batch  1000: 0.51430\n",
      "    Loss after mini-batch  1050: 0.48985\n",
      "    Loss after mini-batch  1100: 0.49534\n",
      "    Loss after mini-batch  1150: 0.49639\n",
      "    Loss after mini-batch  1200: 0.48692\n",
      "    Loss after mini-batch  1250: 0.50072\n",
      "    Loss after mini-batch  1300: 0.46596\n",
      "    Loss after mini-batch  1350: 0.47902\n",
      "    Loss after mini-batch  1400: 0.47826\n",
      "    Loss after mini-batch  1450: 0.51014\n",
      "    Loss after mini-batch  1500: 0.51303\n",
      "    Loss after mini-batch  1550: 0.46810\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.48447\n",
      "    Loss after mini-batch   100: 0.47909\n",
      "    Loss after mini-batch   150: 0.48972\n",
      "    Loss after mini-batch   200: 0.48430\n",
      "    Loss after mini-batch   250: 0.46850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss after mini-batch   300: 0.48072\n",
      "    Loss after mini-batch   350: 0.47524\n",
      "    Loss after mini-batch   400: 0.47279\n",
      "    Loss after mini-batch   450: 0.47948\n",
      "    Loss after mini-batch   500: 0.48792\n",
      "    Loss after mini-batch   550: 0.48250\n",
      "    Loss after mini-batch   600: 0.48013\n",
      "    Loss after mini-batch   650: 0.48316\n",
      "    Loss after mini-batch   700: 0.47780\n",
      "    Loss after mini-batch   750: 0.46713\n",
      "    Loss after mini-batch   800: 0.46507\n",
      "    Loss after mini-batch   850: 0.47164\n",
      "    Loss after mini-batch   900: 0.47332\n",
      "    Loss after mini-batch   950: 0.46618\n",
      "    Loss after mini-batch  1000: 0.49606\n",
      "    Loss after mini-batch  1050: 0.48513\n",
      "    Loss after mini-batch  1100: 0.46669\n",
      "    Loss after mini-batch  1150: 0.46167\n",
      "    Loss after mini-batch  1200: 0.47981\n",
      "    Loss after mini-batch  1250: 0.46548\n",
      "    Loss after mini-batch  1300: 0.48277\n",
      "    Loss after mini-batch  1350: 0.46534\n",
      "    Loss after mini-batch  1400: 0.45932\n",
      "    Loss after mini-batch  1450: 0.47043\n",
      "    Loss after mini-batch  1500: 0.47524\n",
      "    Loss after mini-batch  1550: 0.46401\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.47050\n",
      "    Loss after mini-batch   100: 0.47493\n",
      "    Loss after mini-batch   150: 0.46551\n",
      "    Loss after mini-batch   200: 0.45660\n",
      "    Loss after mini-batch   250: 0.47548\n",
      "    Loss after mini-batch   300: 0.48737\n",
      "    Loss after mini-batch   350: 0.46121\n",
      "    Loss after mini-batch   400: 0.45910\n",
      "    Loss after mini-batch   450: 0.46298\n",
      "    Loss after mini-batch   500: 0.47776\n",
      "    Loss after mini-batch   550: 0.46895\n",
      "    Loss after mini-batch   600: 0.44589\n",
      "    Loss after mini-batch   650: 0.47279\n",
      "    Loss after mini-batch   700: 0.43914\n",
      "    Loss after mini-batch   750: 0.46555\n",
      "    Loss after mini-batch   800: 0.46185\n",
      "    Loss after mini-batch   850: 0.45054\n",
      "    Loss after mini-batch   900: 0.44829\n",
      "    Loss after mini-batch   950: 0.46127\n",
      "    Loss after mini-batch  1000: 0.48429\n",
      "    Loss after mini-batch  1050: 0.45058\n",
      "    Loss after mini-batch  1100: 0.46709\n",
      "    Loss after mini-batch  1150: 0.48116\n",
      "    Loss after mini-batch  1200: 0.45094\n",
      "    Loss after mini-batch  1250: 0.46620\n",
      "    Loss after mini-batch  1300: 0.44025\n",
      "    Loss after mini-batch  1350: 0.45763\n",
      "    Loss after mini-batch  1400: 0.45513\n",
      "    Loss after mini-batch  1450: 0.44893\n",
      "    Loss after mini-batch  1500: 0.45581\n",
      "    Loss after mini-batch  1550: 0.45214\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.45017\n",
      "    Loss after mini-batch   100: 0.45904\n",
      "    Loss after mini-batch   150: 0.44082\n",
      "    Loss after mini-batch   200: 0.45135\n",
      "    Loss after mini-batch   250: 0.45209\n",
      "    Loss after mini-batch   300: 0.45717\n",
      "    Loss after mini-batch   350: 0.43288\n",
      "    Loss after mini-batch   400: 0.45732\n",
      "    Loss after mini-batch   450: 0.44893\n",
      "    Loss after mini-batch   500: 0.46018\n",
      "    Loss after mini-batch   550: 0.44480\n",
      "    Loss after mini-batch   600: 0.44275\n",
      "    Loss after mini-batch   650: 0.44423\n",
      "    Loss after mini-batch   700: 0.46350\n",
      "    Loss after mini-batch   750: 0.45972\n",
      "    Loss after mini-batch   800: 0.43703\n",
      "    Loss after mini-batch   850: 0.46319\n",
      "    Loss after mini-batch   900: 0.44613\n",
      "    Loss after mini-batch   950: 0.45601\n",
      "    Loss after mini-batch  1000: 0.44804\n",
      "    Loss after mini-batch  1050: 0.44883\n",
      "    Loss after mini-batch  1100: 0.44778\n",
      "    Loss after mini-batch  1150: 0.44716\n",
      "    Loss after mini-batch  1200: 0.44508\n",
      "    Loss after mini-batch  1250: 0.44813\n",
      "    Loss after mini-batch  1300: 0.45096\n",
      "    Loss after mini-batch  1350: 0.44418\n",
      "    Loss after mini-batch  1400: 0.44408\n",
      "    Loss after mini-batch  1450: 0.44286\n",
      "    Loss after mini-batch  1500: 0.46045\n",
      "    Loss after mini-batch  1550: 0.45412\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.42284\n",
      "    Loss after mini-batch   100: 0.43413\n",
      "    Loss after mini-batch   150: 0.44430\n",
      "    Loss after mini-batch   200: 0.43587\n",
      "    Loss after mini-batch   250: 0.44095\n",
      "    Loss after mini-batch   300: 0.45789\n",
      "    Loss after mini-batch   350: 0.45299\n",
      "    Loss after mini-batch   400: 0.44922\n",
      "    Loss after mini-batch   450: 0.43155\n",
      "    Loss after mini-batch   500: 0.42116\n",
      "    Loss after mini-batch   550: 0.43483\n",
      "    Loss after mini-batch   600: 0.44126\n",
      "    Loss after mini-batch   650: 0.44437\n",
      "    Loss after mini-batch   700: 0.43597\n",
      "    Loss after mini-batch   750: 0.45606\n",
      "    Loss after mini-batch   800: 0.43579\n",
      "    Loss after mini-batch   850: 0.43463\n",
      "    Loss after mini-batch   900: 0.43014\n",
      "    Loss after mini-batch   950: 0.44744\n",
      "    Loss after mini-batch  1000: 0.43683\n",
      "    Loss after mini-batch  1050: 0.40736\n",
      "    Loss after mini-batch  1100: 0.42428\n",
      "    Loss after mini-batch  1150: 0.44064\n",
      "    Loss after mini-batch  1200: 0.44510\n",
      "    Loss after mini-batch  1250: 0.44945\n",
      "    Loss after mini-batch  1300: 0.47481\n",
      "    Loss after mini-batch  1350: 0.43599\n",
      "    Loss after mini-batch  1400: 0.43386\n",
      "    Loss after mini-batch  1450: 0.43837\n",
      "    Loss after mini-batch  1500: 0.42611\n",
      "    Loss after mini-batch  1550: 0.44594\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 1: 82.64 %\n",
      "F1 for fold 1: 0.83 \n",
      "Runtime for fold 1: 0.0021 s\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.45025\n",
      "    Loss after mini-batch   100: 0.41432\n",
      "    Loss after mini-batch   150: 0.43284\n",
      "    Loss after mini-batch   200: 0.42075\n",
      "    Loss after mini-batch   250: 0.45101\n",
      "    Loss after mini-batch   300: 0.42283\n",
      "    Loss after mini-batch   350: 0.43422\n",
      "    Loss after mini-batch   400: 0.42949\n",
      "    Loss after mini-batch   450: 0.43083\n",
      "    Loss after mini-batch   500: 0.41751\n",
      "    Loss after mini-batch   550: 0.43992\n",
      "    Loss after mini-batch   600: 0.42864\n",
      "    Loss after mini-batch   650: 0.43600\n",
      "    Loss after mini-batch   700: 0.43911\n",
      "    Loss after mini-batch   750: 0.42671\n",
      "    Loss after mini-batch   800: 0.42958\n",
      "    Loss after mini-batch   850: 0.41964\n",
      "    Loss after mini-batch   900: 0.42762\n",
      "    Loss after mini-batch   950: 0.42326\n",
      "    Loss after mini-batch  1000: 0.42671\n",
      "    Loss after mini-batch  1050: 0.42849\n",
      "    Loss after mini-batch  1100: 0.41159\n",
      "    Loss after mini-batch  1150: 0.44559\n",
      "    Loss after mini-batch  1200: 0.41492\n",
      "    Loss after mini-batch  1250: 0.42961\n",
      "    Loss after mini-batch  1300: 0.40781\n",
      "    Loss after mini-batch  1350: 0.42868\n",
      "    Loss after mini-batch  1400: 0.44099\n",
      "    Loss after mini-batch  1450: 0.43349\n",
      "    Loss after mini-batch  1500: 0.42878\n",
      "    Loss after mini-batch  1550: 0.44781\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.42712\n",
      "    Loss after mini-batch   100: 0.43451\n",
      "    Loss after mini-batch   150: 0.42792\n",
      "    Loss after mini-batch   200: 0.42516\n",
      "    Loss after mini-batch   250: 0.40753\n",
      "    Loss after mini-batch   300: 0.43833\n",
      "    Loss after mini-batch   350: 0.42791\n",
      "    Loss after mini-batch   400: 0.41349\n",
      "    Loss after mini-batch   450: 0.42690\n",
      "    Loss after mini-batch   500: 0.42551\n",
      "    Loss after mini-batch   550: 0.40793\n",
      "    Loss after mini-batch   600: 0.40346\n",
      "    Loss after mini-batch   650: 0.42766\n",
      "    Loss after mini-batch   700: 0.42705\n",
      "    Loss after mini-batch   750: 0.42648\n",
      "    Loss after mini-batch   800: 0.43052\n",
      "    Loss after mini-batch   850: 0.41331\n",
      "    Loss after mini-batch   900: 0.41275\n",
      "    Loss after mini-batch   950: 0.42915\n",
      "    Loss after mini-batch  1000: 0.42185\n",
      "    Loss after mini-batch  1050: 0.42391\n",
      "    Loss after mini-batch  1100: 0.42115\n",
      "    Loss after mini-batch  1150: 0.40926\n",
      "    Loss after mini-batch  1200: 0.41202\n",
      "    Loss after mini-batch  1250: 0.43149\n",
      "    Loss after mini-batch  1300: 0.40911\n",
      "    Loss after mini-batch  1350: 0.42502\n",
      "    Loss after mini-batch  1400: 0.41540\n",
      "    Loss after mini-batch  1450: 0.40664\n",
      "    Loss after mini-batch  1500: 0.41305\n",
      "    Loss after mini-batch  1550: 0.41935\n",
      "Starting epoch 3\n",
      "    Loss after mini-batch    50: 0.42445\n",
      "    Loss after mini-batch   100: 0.40402\n",
      "    Loss after mini-batch   150: 0.43015\n",
      "    Loss after mini-batch   200: 0.39838\n",
      "    Loss after mini-batch   250: 0.41565\n",
      "    Loss after mini-batch   300: 0.40892\n",
      "    Loss after mini-batch   350: 0.40914\n",
      "    Loss after mini-batch   400: 0.40986\n",
      "    Loss after mini-batch   450: 0.41068\n",
      "    Loss after mini-batch   500: 0.42019\n",
      "    Loss after mini-batch   550: 0.40873\n",
      "    Loss after mini-batch   600: 0.41480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss after mini-batch   650: 0.41997\n",
      "    Loss after mini-batch   700: 0.41026\n",
      "    Loss after mini-batch   750: 0.40953\n",
      "    Loss after mini-batch   800: 0.41708\n",
      "    Loss after mini-batch   850: 0.41959\n",
      "    Loss after mini-batch   900: 0.42385\n",
      "    Loss after mini-batch   950: 0.40577\n",
      "    Loss after mini-batch  1000: 0.41098\n",
      "    Loss after mini-batch  1050: 0.39186\n",
      "    Loss after mini-batch  1100: 0.42776\n",
      "    Loss after mini-batch  1150: 0.38998\n",
      "    Loss after mini-batch  1200: 0.40439\n",
      "    Loss after mini-batch  1250: 0.41166\n",
      "    Loss after mini-batch  1300: 0.41522\n",
      "    Loss after mini-batch  1350: 0.41273\n",
      "    Loss after mini-batch  1400: 0.40155\n",
      "    Loss after mini-batch  1450: 0.39587\n",
      "    Loss after mini-batch  1500: 0.39228\n",
      "    Loss after mini-batch  1550: 0.41351\n",
      "Starting epoch 4\n",
      "    Loss after mini-batch    50: 0.40122\n",
      "    Loss after mini-batch   100: 0.40926\n",
      "    Loss after mini-batch   150: 0.39708\n",
      "    Loss after mini-batch   200: 0.39881\n",
      "    Loss after mini-batch   250: 0.40043\n",
      "    Loss after mini-batch   300: 0.39419\n",
      "    Loss after mini-batch   350: 0.40942\n",
      "    Loss after mini-batch   400: 0.40324\n",
      "    Loss after mini-batch   450: 0.39362\n",
      "    Loss after mini-batch   500: 0.41283\n",
      "    Loss after mini-batch   550: 0.40768\n",
      "    Loss after mini-batch   600: 0.41097\n",
      "    Loss after mini-batch   650: 0.40652\n",
      "    Loss after mini-batch   700: 0.40214\n",
      "    Loss after mini-batch   750: 0.41250\n",
      "    Loss after mini-batch   800: 0.39793\n",
      "    Loss after mini-batch   850: 0.40907\n",
      "    Loss after mini-batch   900: 0.42597\n",
      "    Loss after mini-batch   950: 0.40240\n",
      "    Loss after mini-batch  1000: 0.39975\n",
      "    Loss after mini-batch  1050: 0.38881\n",
      "    Loss after mini-batch  1100: 0.41880\n",
      "    Loss after mini-batch  1150: 0.40776\n",
      "    Loss after mini-batch  1200: 0.38159\n",
      "    Loss after mini-batch  1250: 0.40675\n",
      "    Loss after mini-batch  1300: 0.39925\n",
      "    Loss after mini-batch  1350: 0.40663\n",
      "    Loss after mini-batch  1400: 0.40135\n",
      "    Loss after mini-batch  1450: 0.38984\n",
      "    Loss after mini-batch  1500: 0.38027\n",
      "    Loss after mini-batch  1550: 0.37371\n",
      "Starting epoch 5\n",
      "    Loss after mini-batch    50: 0.38830\n",
      "    Loss after mini-batch   100: 0.40036\n",
      "    Loss after mini-batch   150: 0.40828\n",
      "    Loss after mini-batch   200: 0.39303\n",
      "    Loss after mini-batch   250: 0.39349\n",
      "    Loss after mini-batch   300: 0.38930\n",
      "    Loss after mini-batch   350: 0.38803\n",
      "    Loss after mini-batch   400: 0.38318\n",
      "    Loss after mini-batch   450: 0.38663\n",
      "    Loss after mini-batch   500: 0.39536\n",
      "    Loss after mini-batch   550: 0.40605\n",
      "    Loss after mini-batch   600: 0.39825\n",
      "    Loss after mini-batch   650: 0.40466\n",
      "    Loss after mini-batch   700: 0.39704\n",
      "    Loss after mini-batch   750: 0.37086\n",
      "    Loss after mini-batch   800: 0.40316\n",
      "    Loss after mini-batch   850: 0.40764\n",
      "    Loss after mini-batch   900: 0.38020\n",
      "    Loss after mini-batch   950: 0.40306\n",
      "    Loss after mini-batch  1000: 0.40075\n",
      "    Loss after mini-batch  1050: 0.39532\n",
      "    Loss after mini-batch  1100: 0.40461\n",
      "    Loss after mini-batch  1150: 0.40910\n",
      "    Loss after mini-batch  1200: 0.38460\n",
      "    Loss after mini-batch  1250: 0.38288\n",
      "    Loss after mini-batch  1300: 0.40473\n",
      "    Loss after mini-batch  1350: 0.39505\n",
      "    Loss after mini-batch  1400: 0.38597\n",
      "    Loss after mini-batch  1450: 0.39272\n",
      "    Loss after mini-batch  1500: 0.39122\n",
      "    Loss after mini-batch  1550: 0.39113\n",
      "Starting testing\n",
      "----------------\n",
      "Accuracy for fold 2: 84.83 %\n",
      "F1 for fold 2: 0.85 \n",
      "Runtime for fold 2: 0.0022 s\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Starting epoch 1\n",
      "    Loss after mini-batch    50: 0.40586\n",
      "    Loss after mini-batch   100: 0.38079\n",
      "    Loss after mini-batch   150: 0.39757\n",
      "    Loss after mini-batch   200: 0.38187\n",
      "    Loss after mini-batch   250: 0.37524\n",
      "    Loss after mini-batch   300: 0.38650\n",
      "    Loss after mini-batch   350: 0.38297\n",
      "    Loss after mini-batch   400: 0.38618\n",
      "    Loss after mini-batch   450: 0.38181\n",
      "    Loss after mini-batch   500: 0.39551\n",
      "    Loss after mini-batch   550: 0.36606\n",
      "    Loss after mini-batch   600: 0.39790\n",
      "    Loss after mini-batch   650: 0.39659\n",
      "    Loss after mini-batch   700: 0.37622\n",
      "    Loss after mini-batch   750: 0.38604\n",
      "    Loss after mini-batch   800: 0.38923\n",
      "    Loss after mini-batch   850: 0.38474\n",
      "    Loss after mini-batch   900: 0.40227\n",
      "    Loss after mini-batch   950: 0.38472\n",
      "    Loss after mini-batch  1000: 0.37999\n",
      "    Loss after mini-batch  1050: 0.39565\n",
      "    Loss after mini-batch  1100: 0.38377\n",
      "    Loss after mini-batch  1150: 0.40446\n",
      "    Loss after mini-batch  1200: 0.38678\n",
      "    Loss after mini-batch  1250: 0.38683\n",
      "    Loss after mini-batch  1300: 0.37520\n",
      "    Loss after mini-batch  1350: 0.37903\n",
      "    Loss after mini-batch  1400: 0.39066\n",
      "    Loss after mini-batch  1450: 0.39184\n",
      "    Loss after mini-batch  1500: 0.38404\n",
      "    Loss after mini-batch  1550: 0.38734\n",
      "Starting epoch 2\n",
      "    Loss after mini-batch    50: 0.38199\n",
      "    Loss after mini-batch   100: 0.39082\n",
      "    Loss after mini-batch   150: 0.38680\n",
      "    Loss after mini-batch   200: 0.37608\n",
      "    Loss after mini-batch   250: 0.36350\n",
      "    Loss after mini-batch   300: 0.38203\n",
      "    Loss after mini-batch   350: 0.39672\n"
     ]
    }
   ],
   "source": [
    "k_folds = 10\n",
    "avg_acc, mean_f1s, mean_runtime = runkfoldcv(\n",
    "    model, dataset, device, k_folds, batch_size, learning_rate, num_epochs, momentum, l2reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef0d6d",
   "metadata": {},
   "source": [
    "### Single fold train & test development code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b766e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data and parameters\n",
    "\n",
    "## Set up Data\n",
    "train_split_percentage = 0.9\n",
    "split_lengths = [int(train_split_percentage*len(dataset)), len(dataset)-int(train_split_percentage*len(dataset))]\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, split_lengths)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_set,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_set,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "## Set up Model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = RFUAVNet(num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2reg, momentum = momentum)  \n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8136219d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.1236\n",
      "Epoch [1/10], Loss: 0.1796\n",
      "Epoch [1/10], Loss: 0.0654\n",
      "Epoch [1/10], Loss: 0.0444\n",
      "Epoch [1/10], Loss: 0.0946\n",
      "Epoch [1/10], Loss: 0.3310\n",
      "Epoch [1/10], Loss: 0.0338\n",
      "Epoch [1/10], Loss: 0.0116\n",
      "Epoch [1/10], Loss: 0.0436\n",
      "Epoch [1/10], Loss: 0.0239\n",
      "Epoch [1/10], Loss: 0.0447\n",
      "Epoch [1/10], Loss: 0.0318\n",
      "Epoch [1/10], Loss: 0.0118\n",
      "Epoch [1/10], Loss: 0.1016\n",
      "Epoch [1/10], Loss: 0.0420\n",
      "Epoch [1/10], Loss: 0.0146\n",
      "Epoch [1/10], Loss: 0.0967\n",
      "Epoch [1/10], Loss: 0.0204\n",
      "Epoch [1/10], Loss: 0.0265\n",
      "Epoch [1/10], Loss: 0.4306\n",
      "Epoch [1/10], Loss: 0.0651\n",
      "Epoch [1/10], Loss: 0.0200\n",
      "Epoch [1/10], Loss: 0.0039\n",
      "Epoch [1/10], Loss: 0.0336\n",
      "Epoch [1/10], Loss: 0.0218\n",
      "Epoch [1/10], Loss: 0.0070\n",
      "Epoch [1/10], Loss: 0.0286\n",
      "Epoch [1/10], Loss: 0.0055\n",
      "Epoch [1/10], Loss: 0.0100\n",
      "Epoch [1/10], Loss: 0.0097\n",
      "Epoch [1/10], Loss: 0.0161\n",
      "Epoch [2/10], Loss: 0.0023\n",
      "Epoch [2/10], Loss: 0.0172\n",
      "Epoch [2/10], Loss: 0.0794\n",
      "Epoch [2/10], Loss: 0.0063\n",
      "Epoch [2/10], Loss: 0.0133\n",
      "Epoch [2/10], Loss: 0.0049\n",
      "Epoch [2/10], Loss: 0.0192\n",
      "Epoch [2/10], Loss: 0.0058\n",
      "Epoch [2/10], Loss: 0.0313\n",
      "Epoch [2/10], Loss: 0.0066\n",
      "Epoch [2/10], Loss: 0.0015\n",
      "Epoch [2/10], Loss: 0.0028\n",
      "Epoch [2/10], Loss: 0.0062\n",
      "Epoch [2/10], Loss: 0.0086\n",
      "Epoch [2/10], Loss: 0.0069\n",
      "Epoch [2/10], Loss: 0.0084\n",
      "Epoch [2/10], Loss: 0.0024\n",
      "Epoch [2/10], Loss: 0.0047\n",
      "Epoch [2/10], Loss: 0.0014\n",
      "Epoch [2/10], Loss: 0.0033\n",
      "Epoch [2/10], Loss: 0.0129\n",
      "Epoch [2/10], Loss: 0.0295\n",
      "Epoch [2/10], Loss: 0.0017\n",
      "Epoch [2/10], Loss: 0.0019\n",
      "Epoch [2/10], Loss: 0.0053\n",
      "Epoch [2/10], Loss: 0.0219\n",
      "Epoch [2/10], Loss: 0.0209\n",
      "Epoch [2/10], Loss: 0.0021\n",
      "Epoch [2/10], Loss: 0.0066\n",
      "Epoch [2/10], Loss: 0.0103\n",
      "Epoch [2/10], Loss: 0.0089\n",
      "Epoch [3/10], Loss: 0.0075\n",
      "Epoch [3/10], Loss: 0.0041\n",
      "Epoch [3/10], Loss: 0.0070\n",
      "Epoch [3/10], Loss: 0.0011\n",
      "Epoch [3/10], Loss: 0.0038\n",
      "Epoch [3/10], Loss: 0.0163\n",
      "Epoch [3/10], Loss: 0.0144\n",
      "Epoch [3/10], Loss: 0.0026\n",
      "Epoch [3/10], Loss: 0.0103\n",
      "Epoch [3/10], Loss: 0.0008\n",
      "Epoch [3/10], Loss: 0.0058\n",
      "Epoch [3/10], Loss: 0.0217\n",
      "Epoch [3/10], Loss: 0.0016\n",
      "Epoch [3/10], Loss: 0.0002\n",
      "Epoch [3/10], Loss: 0.0244\n",
      "Epoch [3/10], Loss: 0.0017\n",
      "Epoch [3/10], Loss: 0.0063\n",
      "Epoch [3/10], Loss: 0.0397\n",
      "Epoch [3/10], Loss: 0.0245\n",
      "Epoch [3/10], Loss: 0.0017\n",
      "Epoch [3/10], Loss: 0.0048\n",
      "Epoch [3/10], Loss: 0.0049\n",
      "Epoch [3/10], Loss: 0.0019\n",
      "Epoch [3/10], Loss: 0.0051\n",
      "Epoch [3/10], Loss: 0.0067\n",
      "Epoch [3/10], Loss: 0.0098\n",
      "Epoch [3/10], Loss: 0.0016\n",
      "Epoch [3/10], Loss: 0.0199\n",
      "Epoch [3/10], Loss: 0.0024\n",
      "Epoch [3/10], Loss: 0.0033\n",
      "Epoch [3/10], Loss: 0.0030\n",
      "Epoch [4/10], Loss: 0.0029\n",
      "Epoch [4/10], Loss: 0.0031\n",
      "Epoch [4/10], Loss: 0.0123\n",
      "Epoch [4/10], Loss: 0.0019\n",
      "Epoch [4/10], Loss: 0.0023\n",
      "Epoch [4/10], Loss: 0.0207\n",
      "Epoch [4/10], Loss: 0.0044\n",
      "Epoch [4/10], Loss: 0.0018\n",
      "Epoch [4/10], Loss: 0.0074\n",
      "Epoch [4/10], Loss: 0.0051\n",
      "Epoch [4/10], Loss: 0.0056\n",
      "Epoch [4/10], Loss: 0.0197\n",
      "Epoch [4/10], Loss: 0.0172\n",
      "Epoch [4/10], Loss: 0.0041\n",
      "Epoch [4/10], Loss: 0.0050\n",
      "Epoch [4/10], Loss: 0.0048\n",
      "Epoch [4/10], Loss: 0.0270\n",
      "Epoch [4/10], Loss: 0.0031\n",
      "Epoch [4/10], Loss: 0.0006\n",
      "Epoch [4/10], Loss: 0.0020\n",
      "Epoch [4/10], Loss: 0.0011\n",
      "Epoch [4/10], Loss: 0.0068\n",
      "Epoch [4/10], Loss: 0.0248\n",
      "Epoch [4/10], Loss: 0.0068\n",
      "Epoch [4/10], Loss: 0.0017\n",
      "Epoch [4/10], Loss: 0.0013\n",
      "Epoch [4/10], Loss: 0.0030\n",
      "Epoch [4/10], Loss: 0.0022\n",
      "Epoch [4/10], Loss: 0.0021\n",
      "Epoch [4/10], Loss: 0.0028\n",
      "Epoch [4/10], Loss: 0.0011\n",
      "Epoch [5/10], Loss: 0.0133\n",
      "Epoch [5/10], Loss: 0.0015\n",
      "Epoch [5/10], Loss: 0.0004\n",
      "Epoch [5/10], Loss: 0.0013\n",
      "Epoch [5/10], Loss: 0.0043\n",
      "Epoch [5/10], Loss: 0.0198\n",
      "Epoch [5/10], Loss: 0.0262\n",
      "Epoch [5/10], Loss: 0.0022\n",
      "Epoch [5/10], Loss: 0.0081\n",
      "Epoch [5/10], Loss: 0.0079\n",
      "Epoch [5/10], Loss: 0.0003\n",
      "Epoch [5/10], Loss: 0.0060\n",
      "Epoch [5/10], Loss: 0.0074\n",
      "Epoch [5/10], Loss: 0.0025\n",
      "Epoch [5/10], Loss: 0.0003\n",
      "Epoch [5/10], Loss: 0.0044\n",
      "Epoch [5/10], Loss: 0.0044\n",
      "Epoch [5/10], Loss: 0.0087\n",
      "Epoch [5/10], Loss: 0.0063\n",
      "Epoch [5/10], Loss: 0.0100\n",
      "Epoch [5/10], Loss: 0.0031\n",
      "Epoch [5/10], Loss: 0.0018\n",
      "Epoch [5/10], Loss: 0.0034\n",
      "Epoch [5/10], Loss: 0.0034\n",
      "Epoch [5/10], Loss: 0.0059\n",
      "Epoch [5/10], Loss: 0.0024\n",
      "Epoch [5/10], Loss: 0.0065\n",
      "Epoch [5/10], Loss: 0.0148\n",
      "Epoch [5/10], Loss: 0.0087\n",
      "Epoch [5/10], Loss: 0.0008\n",
      "Epoch [5/10], Loss: 0.0021\n",
      "Epoch [6/10], Loss: 0.0011\n",
      "Epoch [6/10], Loss: 0.0008\n",
      "Epoch [6/10], Loss: 0.0088\n",
      "Epoch [6/10], Loss: 0.0065\n",
      "Epoch [6/10], Loss: 0.0010\n",
      "Epoch [6/10], Loss: 0.0016\n",
      "Epoch [6/10], Loss: 0.0104\n",
      "Epoch [6/10], Loss: 0.0035\n",
      "Epoch [6/10], Loss: 0.0004\n",
      "Epoch [6/10], Loss: 0.0007\n",
      "Epoch [6/10], Loss: 0.0120\n",
      "Epoch [6/10], Loss: 0.0006\n",
      "Epoch [6/10], Loss: 0.0042\n",
      "Epoch [6/10], Loss: 0.0017\n",
      "Epoch [6/10], Loss: 0.0045\n",
      "Epoch [6/10], Loss: 0.0083\n",
      "Epoch [6/10], Loss: 0.0022\n",
      "Epoch [6/10], Loss: 0.0026\n",
      "Epoch [6/10], Loss: 0.0050\n",
      "Epoch [6/10], Loss: 0.0028\n",
      "Epoch [6/10], Loss: 0.0003\n",
      "Epoch [6/10], Loss: 0.0007\n",
      "Epoch [6/10], Loss: 0.0003\n",
      "Epoch [6/10], Loss: 0.0027\n",
      "Epoch [6/10], Loss: 0.0015\n",
      "Epoch [6/10], Loss: 0.0002\n",
      "Epoch [6/10], Loss: 0.0051\n",
      "Epoch [6/10], Loss: 0.0003\n",
      "Epoch [6/10], Loss: 0.0034\n",
      "Epoch [6/10], Loss: 0.0017\n",
      "Epoch [6/10], Loss: 0.0005\n",
      "Epoch [7/10], Loss: 0.0004\n",
      "Epoch [7/10], Loss: 0.0006\n",
      "Epoch [7/10], Loss: 0.0170\n",
      "Epoch [7/10], Loss: 0.0080\n",
      "Epoch [7/10], Loss: 0.0009\n",
      "Epoch [7/10], Loss: 0.0007\n",
      "Epoch [7/10], Loss: 0.0059\n",
      "Epoch [7/10], Loss: 0.0008\n",
      "Epoch [7/10], Loss: 0.0061\n",
      "Epoch [7/10], Loss: 0.0003\n",
      "Epoch [7/10], Loss: 0.0023\n",
      "Epoch [7/10], Loss: 0.0014\n",
      "Epoch [7/10], Loss: 0.0037\n",
      "Epoch [7/10], Loss: 0.0007\n",
      "Epoch [7/10], Loss: 0.0013\n",
      "Epoch [7/10], Loss: 0.0007\n",
      "Epoch [7/10], Loss: 0.0036\n",
      "Epoch [7/10], Loss: 0.0005\n",
      "Epoch [7/10], Loss: 0.0024\n",
      "Epoch [7/10], Loss: 0.0081\n",
      "Epoch [7/10], Loss: 0.0008\n",
      "Epoch [7/10], Loss: 0.0031\n",
      "Epoch [7/10], Loss: 0.0012\n",
      "Epoch [7/10], Loss: 0.0014\n",
      "Epoch [7/10], Loss: 0.0008\n",
      "Epoch [7/10], Loss: 0.0016\n",
      "Epoch [7/10], Loss: 0.0019\n",
      "Epoch [7/10], Loss: 0.0056\n",
      "Epoch [7/10], Loss: 0.0007\n",
      "Epoch [7/10], Loss: 0.0028\n",
      "Epoch [7/10], Loss: 0.0003\n",
      "Epoch [8/10], Loss: 0.0010\n",
      "Epoch [8/10], Loss: 0.0005\n",
      "Epoch [8/10], Loss: 0.0008\n",
      "Epoch [8/10], Loss: 0.0003\n",
      "Epoch [8/10], Loss: 0.0014\n",
      "Epoch [8/10], Loss: 0.0022\n",
      "Epoch [8/10], Loss: 0.0271\n",
      "Epoch [8/10], Loss: 0.0022\n",
      "Epoch [8/10], Loss: 0.0019\n",
      "Epoch [8/10], Loss: 0.0224\n",
      "Epoch [8/10], Loss: 0.0466\n",
      "Epoch [8/10], Loss: 0.0004\n",
      "Epoch [8/10], Loss: 0.0019\n",
      "Epoch [8/10], Loss: 0.0362\n",
      "Epoch [8/10], Loss: 0.0003\n",
      "Epoch [8/10], Loss: 0.0030\n",
      "Epoch [8/10], Loss: 0.0020\n",
      "Epoch [8/10], Loss: 0.0006\n",
      "Epoch [8/10], Loss: 0.0024\n",
      "Epoch [8/10], Loss: 0.0037\n",
      "Epoch [8/10], Loss: 0.0013\n",
      "Epoch [8/10], Loss: 0.0006\n",
      "Epoch [8/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0002\n",
      "Epoch [8/10], Loss: 0.0004\n",
      "Epoch [8/10], Loss: 0.0059\n",
      "Epoch [8/10], Loss: 0.0010\n",
      "Epoch [8/10], Loss: 0.0028\n",
      "Epoch [8/10], Loss: 0.0032\n",
      "Epoch [8/10], Loss: 0.0003\n",
      "Epoch [8/10], Loss: 0.0045\n",
      "Epoch [9/10], Loss: 0.0111\n",
      "Epoch [9/10], Loss: 0.0035\n",
      "Epoch [9/10], Loss: 0.0003\n",
      "Epoch [9/10], Loss: 0.0018\n",
      "Epoch [9/10], Loss: 0.0039\n",
      "Epoch [9/10], Loss: 0.0003\n",
      "Epoch [9/10], Loss: 0.0004\n",
      "Epoch [9/10], Loss: 0.0016\n",
      "Epoch [9/10], Loss: 0.0029\n",
      "Epoch [9/10], Loss: 0.0025\n",
      "Epoch [9/10], Loss: 0.0132\n",
      "Epoch [9/10], Loss: 0.0167\n",
      "Epoch [9/10], Loss: 0.0156\n",
      "Epoch [9/10], Loss: 0.0014\n",
      "Epoch [9/10], Loss: 0.0286\n",
      "Epoch [9/10], Loss: 0.0043\n",
      "Epoch [9/10], Loss: 0.0009\n",
      "Epoch [9/10], Loss: 0.0010\n",
      "Epoch [9/10], Loss: 0.0007\n",
      "Epoch [9/10], Loss: 0.0042\n",
      "Epoch [9/10], Loss: 0.0022\n",
      "Epoch [9/10], Loss: 0.0060\n",
      "Epoch [9/10], Loss: 0.0007\n",
      "Epoch [9/10], Loss: 0.0128\n",
      "Epoch [9/10], Loss: 0.0022\n",
      "Epoch [9/10], Loss: 0.0009\n",
      "Epoch [9/10], Loss: 0.0011\n",
      "Epoch [9/10], Loss: 0.0063\n",
      "Epoch [9/10], Loss: 0.0028\n",
      "Epoch [9/10], Loss: 0.0015\n",
      "Epoch [9/10], Loss: 0.0073\n",
      "Epoch [10/10], Loss: 0.0021\n",
      "Epoch [10/10], Loss: 0.0086\n",
      "Epoch [10/10], Loss: 0.0006\n",
      "Epoch [10/10], Loss: 0.0051\n",
      "Epoch [10/10], Loss: 0.0006\n",
      "Epoch [10/10], Loss: 0.0005\n",
      "Epoch [10/10], Loss: 0.0014\n",
      "Epoch [10/10], Loss: 0.0011\n",
      "Epoch [10/10], Loss: 0.0054\n",
      "Epoch [10/10], Loss: 0.0040\n",
      "Epoch [10/10], Loss: 0.0016\n",
      "Epoch [10/10], Loss: 0.0012\n",
      "Epoch [10/10], Loss: 0.0015\n",
      "Epoch [10/10], Loss: 0.0049\n",
      "Epoch [10/10], Loss: 0.0006\n",
      "Epoch [10/10], Loss: 0.0006\n",
      "Epoch [10/10], Loss: 0.0002\n",
      "Epoch [10/10], Loss: 0.0006\n",
      "Epoch [10/10], Loss: 0.0004\n",
      "Epoch [10/10], Loss: 0.0002\n",
      "Epoch [10/10], Loss: 0.0001\n",
      "Epoch [10/10], Loss: 0.0004\n",
      "Epoch [10/10], Loss: 0.0054\n",
      "Epoch [10/10], Loss: 0.0026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.0039\n",
      "Epoch [10/10], Loss: 0.0004\n",
      "Epoch [10/10], Loss: 0.0047\n",
      "Epoch [10/10], Loss: 0.0006\n",
      "Epoch [10/10], Loss: 0.0044\n",
      "Epoch [10/10], Loss: 0.0011\n",
      "Epoch [10/10], Loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "    #Load in the data in batches using the train_loader object\n",
    "    for i, (inputs, labels) in enumerate(train_loader): \n",
    "#         inputs = inputs.float()\n",
    "#         inputs = torch.squeeze(inputs, 1)\n",
    "        labels = labels.type(torch.long)\n",
    "\n",
    "        # Move tensors to the configured device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%50 == 49:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "953e1204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 22600 train images: 99.86725663716814 %\n"
     ]
    }
   ],
   "source": [
    "## Check accuracy\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        inputs = torch.squeeze(inputs, 1)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "#         print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print('Accuracy of the network on the {} train images: {} %'.format(total, 100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc778345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # KFoldCV original implementation\n",
    "# k_folds = 10\n",
    "\n",
    "# num_classes =10\n",
    "\n",
    "# if num_classes == 2:\n",
    "#     f1type = 'binary'\n",
    "# else:\n",
    "#     f1type = 'weighted' # is this the best choice\n",
    "\n",
    "\n",
    "# # For fold results\n",
    "# results = {}\n",
    "# runtimes = np.zeros(k_folds)\n",
    "# f1s = np.zeros(k_folds)\n",
    "\n",
    "# # Define the K-fold Cross Validator\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# # Start print\n",
    "# print('--------------------------------')\n",
    "\n",
    "# # K-fold Cross Validation model evaluation\n",
    "# for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "#     # Print\n",
    "#     print(f'FOLD {fold}')\n",
    "#     print('--------------------------------')\n",
    "\n",
    "#     # Sample elements randomly from a given list of ids, no replacement.\n",
    "#     train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "#     test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "#     # Define data loaders for training and testing data in this fold\n",
    "#     trainloader = torch.utils.data.DataLoader(\n",
    "#                       dataset, \n",
    "#                       batch_size=batch_size, sampler=train_subsampler)\n",
    "#     testloader = torch.utils.data.DataLoader(\n",
    "#                       dataset,\n",
    "#                       batch_size=batch_size, sampler=test_subsampler)\n",
    "\n",
    "#     # Init the neural network\n",
    "#     network = RFUAVNet(num_classes)\n",
    "#     network = network.to(device)\n",
    "# #     network.apply(reset_weights)\n",
    "\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     # Initialize optimizer\n",
    "# #     optimizer = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "#     optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, weight_decay=l2reg, momentum = momentum)  \n",
    "\n",
    "#     # Run the training loop for defined number of epochs\n",
    "#     for epoch in range(0, num_epochs):\n",
    "#         # Print epoch\n",
    "#         print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "#         # Set current loss value\n",
    "#         current_loss = 0.0\n",
    "\n",
    "#         # Iterate over the DataLoader for training data\n",
    "#         for i, data in enumerate(trainloader):\n",
    "#             # Get inputs\n",
    "#             inputs, targets = data\n",
    "#             targets= targets.type(torch.long)\n",
    "                        \n",
    "#             # Move tensors to configured device\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets = targets.to(device)\n",
    "            \n",
    "#             # Perform forward pass\n",
    "#             outputs = network(inputs)\n",
    "            \n",
    "#             # Compute loss            \n",
    "#             loss = criterion(outputs, targets)\n",
    "\n",
    "#             # Zero the gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Perform backward pass\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Perform optimization\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Print statistics\n",
    "#             current_loss += loss.item()\n",
    "#             if i % 50 == 49:\n",
    "#                 print('    Loss after mini-batch %5d: %.5f' %\n",
    "#                       (i + 1, current_loss / 50))\n",
    "#                 current_loss = 0.0\n",
    "# #         print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "#     # Process is complete.\n",
    "# #     print('Training process has finished. Saving trained model.')\n",
    "\n",
    "#     # Print about testing\n",
    "#     print('Starting testing')\n",
    "#     print('----------------')\n",
    "\n",
    "#     # Saving the model\n",
    "# #     save_path = f'./model-fold-{fold}.pth'\n",
    "# #     torch.save(network.state_dict(), save_path)\n",
    "\n",
    "#     # Evaluation for this fold\n",
    "#     correct, total = 0, 0\n",
    "#     network.eval()\n",
    "#     with torch.no_grad():\n",
    "#         starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "#         runtimes_thisfold = []\n",
    "#         f1s_thisfold = []\n",
    "#         # Iterate over the test data and generate predictions\n",
    "#         for i, data in enumerate(testloader, 0):\n",
    "#             # Get inputs\n",
    "#             inputs, targets = data\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets = targets.to(device)\n",
    "\n",
    "#             # Generate outputs\n",
    "#             n_instances = len(inputs)\n",
    "#             ys = torch.empty(n_instances)\n",
    "#             ys = ys.to(device)\n",
    "\n",
    "#             for i in range(n_instances):\n",
    "#                 instance = inputs[i]\n",
    "#                 instance = instance.float()\n",
    "#                 start = time.time()\n",
    "#                 starter.record()\n",
    "#                 yi = network(instance)\n",
    "#                 _,pred = torch.max(yi,1)\n",
    "#                 ender.record()\n",
    "                \n",
    "#                 torch.cuda.synchronize()\n",
    "#                 curr_time = starter.elapsed_time(ender) #miliseconds\n",
    "\n",
    "#                 runtimes_thisfold.append(curr_time*1e-3)\n",
    "#                 ys[i] = pred\n",
    "\n",
    "\n",
    "#             # Set total and correct\n",
    "#             total += targets.size(0)\n",
    "#             correct += (ys == targets).sum().item()\n",
    "#             f1i = f1_score(ys.cpu().numpy(), targets.cpu().numpy(), average=f1type)\n",
    "#             f1s_thisfold.append(f1i)\n",
    "            \n",
    "#         mean_runtime = np.mean(np.array(runtimes_thisfold))\n",
    "#         mean_f1 = np.mean(np.array(f1s_thisfold))\n",
    "\n",
    "#     # Summarize and print results\n",
    "#     results[fold] = 100.0 * (correct / total)\n",
    "#     runtimes[fold] = mean_runtime\n",
    "#     f1s[fold] = mean_f1\n",
    "#     print('Accuracy for fold %d: %.2f %%' % (fold, 100.0 * correct / total))\n",
    "#     print('F1 for fold %d: %.2f ' % (fold, mean_f1))\n",
    "#     print('Runtime for fold %d: %.4f s' % (fold, mean_runtime))\n",
    "#     print('--------------------------------')\n",
    "\n",
    "# # Print fold results\n",
    "# print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
    "# print('--------------------------------')\n",
    "# sum = 0.0\n",
    "# for key, value in results.items():\n",
    "#     print(f'Fold {key}: {value} %')\n",
    "#     sum += value\n",
    "# print(f'Average Accuracy: {sum/len(results.items())} %')\n",
    "# print(f'Average F1: {np.mean(f1s)}')\n",
    "# print(f'Average Runtime: {np.mean(runtimes)} s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
